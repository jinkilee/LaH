{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:\n",
    "- https://github.com/jungyeul/korean-parallel-corpora\n",
    "- https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concatenate kor.txt and eng.txt with tab delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../KoGPT2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import tqdm\n",
    "import sklearn\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotly.offline import iplot\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kogpt2.utils import get_tokenizer\n",
    "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../korean-parallel-corpora/korean-english-news-v1'\n",
    "targets = ['train', 'test', 'dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have ../korean-parallel-corpora/korean-english-news-v1/train.txt\n",
      "already have ../korean-parallel-corpora/korean-english-news-v1/test.txt\n",
      "already have ../korean-parallel-corpora/korean-english-news-v1/dev.txt\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    output_filename = '{}/{}.txt'.format(data_dir, target)\n",
    "    if os.path.exists(output_filename):\n",
    "        print('already have {}'.format(output_filename))\n",
    "        continue\n",
    "        \n",
    "    ko_file, en_file = glob.glob('{}/*{}.??'.format(data_dir, target))\n",
    "    \n",
    "    # read korean/english files\n",
    "    ko_lines = open(ko_file).read().strip().split('\\n')\n",
    "    en_lines = open(en_file).read().strip().split('\\n')\n",
    "    \n",
    "    # write to output file\n",
    "    with open(output_filename, 'w') as out:\n",
    "        for en, kr in zip(en_lines, ko_lines):\n",
    "            oneline = '\\t'.join([en, kr])\n",
    "            out.write(oneline + '\\n')\n",
    "            \n",
    "    print('{} was written'.format(output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_nm)\n",
    "eng_tokenizer = tokenizer.tokenize\n",
    "\n",
    "embedding_size = 10\n",
    "hidden_size = 32\n",
    "n_batch = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tok_path = get_tokenizer()\n",
    "kor_tokenizer = SentencepieceTokenizer(tok_path)\n",
    "_, vocab = get_pytorch_kogpt2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['<pad>'])[0], tokenizer.vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_index(kr=None, en=None, kr_pad=tokenizer.vocab['[PAD]'], en_pad=vocab(['<pad>'])[0], maxlen=50):\n",
    "    '''\n",
    "        convert korean/english sentence into its own indices.\n",
    "        maximum length of converted indices is `maxlen`\n",
    "    '''\n",
    "    assert (kr != None) or (en != None), 'one of either kr or en should have a value'\n",
    "    kr_index, en_index = None, None\n",
    "    \n",
    "    if kr:\n",
    "        kr_index = vocab(kor_tokenizer(kr))\n",
    "        if len(kr_index) > maxlen:\n",
    "            kr_index = kr_index[:maxlen]\n",
    "        else:\n",
    "            kr_index = kr_index + [kr_pad] * (maxlen-len(kr_index))\n",
    "    if en:\n",
    "        en_index = tokenizer.convert_tokens_to_ids(eng_tokenizer(en))\n",
    "        if len(en_index) > maxlen:\n",
    "            en_index = en_index[:maxlen]\n",
    "        else:\n",
    "            en_index = en_index + [en_pad] * (maxlen-len(en_index))\n",
    "        \n",
    "    return kr_index, en_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationText():\n",
    "    def __init__(self, kr, en):\n",
    "        self.kr = kr\n",
    "        self.en = en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Translation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None, names=['kor', 'eng'], sep='\\t'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            names (list): A list of column names\n",
    "            sep (string): A string that is used for a delimiter.\n",
    "        \"\"\"\n",
    "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.df = pd.read_csv(csv_file, names=names, sep=sep)\n",
    "        self.df = self.df[(self.df.kor.notnull()) & (self.df.eng.notnull())]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        onerow =  self.df.iloc[idx]\n",
    "        kr_index, en_index = convert_string_to_index(kr=onerow.kor, en=onerow.eng)\n",
    "        kor_tensor = torch.LongTensor(kr_index)\n",
    "        eng_tensor = torch.LongTensor(en_index)\n",
    "        return (kor_tensor, eng_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TranslationDataset('../korean-parallel-corpora/korean-english-news-v1/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset, batch_size = n_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50])\n",
      "torch.Size([8, 50])\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i, (input_kr, input_en) in enumerate(trainloader):\n",
    "    print(input_kr.shape)\n",
    "    print(input_en.shape)\n",
    "    print('------------')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 50]), torch.Size([8, 50]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_kr.shape, input_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_batch):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, self.embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, \n",
    "                          hidden_size,\n",
    "                          num_layers=1,\n",
    "                          bias=True,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=False)\n",
    "        self.init_hidden = torch.zeros(1, n_batch, self.hidden_size).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.gru(x, self.init_hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_batch):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size+hidden_size,\n",
    "                         hidden_size,\n",
    "                         num_layers=1,\n",
    "                         bias=True, \n",
    "                         batch_first=True,\n",
    "                         bidirectional=False)\n",
    "        self.init_hidden = torch.zeros(1, n_batch, self.hidden_size).cuda()\n",
    "        self.tahn = nn.Tanh()\n",
    "        \n",
    "        self.W1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.W2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.V = nn.Linear(self.hidden_size, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.fc = nn.Linear(self.hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        score = self.V(self.tahn(self.W1(enc_output) + self.W2(hidden)))\n",
    "        attention_weights = self.softmax(score)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = context_vector.sum(dim=1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = torch.cat((x, context_vector.unsqueeze(dim=1)), dim=-1)\n",
    "        output, state = self.gru(x, self.init_hidden)\n",
    "        output = output.squeeze(dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, src_vocab_size, dst_vocab_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # tokenizer.vocab_size == src_vocab_size\n",
    "        # len(vocab) == dst_vocab_size\n",
    "        self.encoder = Encoder(src_vocab_size, embedding_size, hidden_size, n_batch=n_batch)\n",
    "        self.decoder = Decoder(dst_vocab_size, embedding_size, hidden_size, n_batch=n_batch)\n",
    "        self.loss_fn = DecoderLoss()\n",
    "        \n",
    "    def forward(self, src, dst):\n",
    "        enc_output, enc_hidden = self.encoder(src)\n",
    "\n",
    "        loss = 0\n",
    "        hidden = enc_hidden.transpose(0, 1)\n",
    "        dec_input = torch.LongTensor([vocab(['<start>'])] * n_batch).to('cuda')\n",
    "        for t in range(1, dst.size()[1]):\n",
    "            output, state, _ = self.decoder(dec_input, hidden, enc_output)\n",
    "            loss += self.loss_fn(dst[:, t], output)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, real, pred):\n",
    "        mask = 1 - (real == 0).type(torch.LongTensor)\n",
    "        loss = self.loss_fn(pred, real) * mask\n",
    "        loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_kr.shape, input_en.shape\n",
    "torch.cat((input_kr, input_en), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30522, 10)\n",
       "    (gru): GRU(10, 32, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(50000, 10)\n",
       "    (gru): GRU(42, 32, batch_first=True)\n",
       "    (tahn): Tanh()\n",
       "    (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (fc): Linear(in_features=32, out_features=50000, bias=True)\n",
       "  )\n",
       "  (loss_fn): DecoderLoss(\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq = EncoderDecoder(embedding_size, hidden_size, tokenizer.vocab_size, len(vocab))\n",
    "seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30522, 10)\n",
       "    (gru): GRU(10, 32, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(50000, 10)\n",
       "    (gru): GRU(42, 32, batch_first=True)\n",
       "    (tahn): Tanh()\n",
       "    (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (fc): Linear(in_features=32, out_features=50000, bias=True)\n",
       "  )\n",
       "  (loss_fn): DecoderLoss(\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training at 0th epoch: 34it [00:01, 19.40it/s, loss=tensor(43.3575, grad_fn=<DivBackward0>)]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.93 GiB total capacity; 5.43 GiB already allocated; 5.75 MiB free; 5.44 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-88280915940f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minput_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minput_kr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_kr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_kr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#         enc_output, enc_hidden = encoder(input_en)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-da1544dd252d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, dst)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-36b712f997ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.93 GiB total capacity; 5.43 GiB already allocated; 5.75 MiB free; 5.44 GiB reserved in total by PyTorch)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training at 0th epoch: 34it [00:20, 19.40it/s, loss=tensor(43.3575, grad_fn=<DivBackward0>)]"
     ]
    }
   ],
   "source": [
    "n_epoch = 5\n",
    "for epoch in range(n_epoch):\n",
    "    total_loss = 0\n",
    "    \n",
    "    tbar = tqdm(enumerate(trainloader), desc='training at {}th epoch'.format(i))\n",
    "    for i, (input_kr, input_en) in tbar:\n",
    "        input_en = input_en.cuda()\n",
    "        input_kr = input_kr.cuda()\n",
    "        loss = seq2seq(input_en, input_kr)\n",
    "#         enc_output, enc_hidden = encoder(input_en)\n",
    "\n",
    "#         loss = 0\n",
    "#         hidden = enc_hidden.transpose(0, 1)\n",
    "#         dec_input = torch.LongTensor([vocab(['<pad>'])] * n_batch)\n",
    "#         for t in range(1, input_kr.size()[1]):\n",
    "#             output, state, _ = decoder(dec_input, hidden, enc_output)\n",
    "#             loss += loss_fn(input_kr[:, t], output)\n",
    "        \n",
    "        # calculate loss\n",
    "        batch_loss = loss / input_kr.size()[1]\n",
    "        total_loss += batch_loss\n",
    "        tbar.set_postfix(loss=batch_loss)\n",
    "        \n",
    "    print('Loss at {}th epoch: {:.4f}'.format(epoch, total_loss / n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 10])\n",
      "torch.Size([2, 3, 30])\n",
      "----\n",
      "torch.Size([3, 4, 10])\n",
      "torch.Size([3, 4, 60])\n",
      "torch.Size([2, 3, 30])\n"
     ]
    }
   ],
   "source": [
    "input_size = 10 # input dimension (word embedding) D\n",
    "hidden_size = 30 # hidden dimension H\n",
    "batch_size = 3\n",
    "length = 4\n",
    "\n",
    "rnn = nn.GRU(input_size,hidden_size,num_layers=1,bias=True,batch_first=True,bidirectional=True)\n",
    "inputs = Variable(torch.randn(batch_size,length,input_size)) # B,T,D\n",
    "hidden = Variable(torch.zeros(2,batch_size,hidden_size)) # 2,B,H\n",
    "\n",
    "print(inputs.shape)\n",
    "print(hidden.size())\n",
    "print('----')\n",
    "output, hidden = rnn(inputs, hidden)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(output.size())\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path\n",
    "file_path = os.path.join(data_dir, 'train.txt')\n",
    "\n",
    "# read the file\n",
    "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
    "lines = lines[:20000]\n",
    "\n",
    "# perform basic cleaning\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "remove_digits = str.maketrans('', '', string.digits) # Set of all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kor_sentence(sent):\n",
    "    '''Function to preprocess Marathi sentence'''\n",
    "    sent = re.sub(\"'\", '', sent)\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent\n",
    "\n",
    "def preprocess_eng_sentence(sent):\n",
    "    '''Function to preprocess English sentence'''\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(\"'\", '', sent)\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits)\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairs of cleaned English and Marathi sentences\n",
    "sent_pairs = []\n",
    "for line in lines:\n",
    "    sent_pair = []\n",
    "    ko, en = line.split('\\t')\n",
    "    \n",
    "    # append korean\n",
    "    ko = preprocess_kor_sentence(ko)\n",
    "    sent_pair.append(ko)\n",
    "    \n",
    "    # append english\n",
    "    en = preprocess_kor_sentence(en)\n",
    "    sent_pair.append(en)\n",
    "    \n",
    "    # append sentence pair\n",
    "    sent_pairs.append(sent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate maximum length of the sequence\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(pairs, num_examples):\n",
    "    # pairs => already created cleaned input, output pairs\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
    "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # English sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # Marathi sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tensors\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters of the model\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch generator to be used by modle to load data in batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        # this is the step 1 described in the blog to compute scores s1, s2, ...\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # this is the step 2 described in the blog to compute attention weights e1, e2, ...\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # this is the step 3 described in the blog to compute the context_vector = e1*h1 + e2*h2 + ...\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # this is the step 4 described in the blog to concatenate the context vector with the output of the previous time step\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        # this is the step 5 in the blog, to compute the next output word in the sequence\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        # return current output, current state and the attention weights\n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-84cdabcc053e>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create objects of Class Encoder and Class Decoder\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - (real == 0).type(torch.LongTensor)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6ce3518e78cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}/training_checkpoints'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                  \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  decoder=decoder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = '{}/training_checkpoints'.format(data_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "training 0th epoch: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (64, 69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training 1th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 0th epoch: 0it [00:06, ?it/s]\n",
      "\n",
      "training 2th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 1th epoch: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 1 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 2 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training 3th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 2th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 4th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 3th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 5th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 4th epoch: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 4 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 5 Loss 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training 6th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 5th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 7th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 6th epoch: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 6 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 7 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training 8th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 7th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 9th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 8th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 10th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 9th epoch: 0it [00:00, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 9 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 10 Loss 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "training 11th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 10th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 12th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 11th epoch: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 11 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 12 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training 13th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 12th epoch: 0it [00:00, ?it/s]\n",
      "\n",
      "training 14th epoch: 0it [00:00, ?it/s]\u001b[A\n",
      "training 13th epoch: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 14 Loss 0.0000\n",
      "input shape: (64, 69)\n",
      "encoder output shape: (64, 69, 256)\n",
      "encoder hidden shape: (64, 256)\n",
      "---------------\n",
      "Epoch 15 Loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    tbar = tqdm.tqdm(enumerate(dataset), desc='training {}th epoch'.format(epoch))\n",
    "    for (batch, (inp, targ)) in tbar:\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            print('input shape: {}'.format(inp.shape))\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            print('encoder output shape: {}'.format(enc_output.shape))\n",
    "            print('encoder hidden shape: {}'.format(enc_hidden.shape))\n",
    "            print('---------------')\n",
    "            break\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        tbar.set_postfix(loss=batch_loss)\n",
    "        '''\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "        '''\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
    "    #print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
    "    sentence = sentence[:-1]\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    # start decoding\n",
    "    for t in range(max_length_targ): # limit the length of the decoded sequence\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        # stop decoding if '<end>' is predicted\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_val_sentence():\n",
    "    actual_sent = ''\n",
    "    k = np.random.randint(len(input_tensor_val))\n",
    "    random_input = input_tensor_val[k]\n",
    "    random_output = target_tensor_val[k]\n",
    "    random_input = np.expand_dims(random_input,0)\n",
    "    \n",
    "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "    print('Input: {}'.format(sentence[8:-6]))\n",
    "    print('Predicted translation: {}'.format(result[:-6]))\n",
    "    for i in random_output:\n",
    "        if i == 0:\n",
    "            break\n",
    "        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "    actual_sent = actual_sent[8:-7]\n",
    "    print('Actual translation: {}'.format(actual_sent))\n",
    "    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
    "    sentence, result = sentence.split(' '), result.split(' ')\n",
    "    sentence = sentence[1:-1]\n",
    "    result = result[:-2]\n",
    "    # use plotly to plot the heatmap\n",
    "    #trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='Reds')\n",
    "    #data=[trace]\n",
    "    #iplot(data)\n",
    "    return attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: As the yearend employment season is approaching college graduatestobe or graduates are busy seeking jobs but their rate of employment at large enterprises remains at a low level\n",
      "Predicted translation: 대선레이스에 바카스를 마련했지만 샌안젤로 예정이지만 버클리 중동국가는 스탈린에 경험했던 헤엄을 달러가 프로스트 계획”이라며 화성탐사선과의 일년 입법권에 닷오알지org 소요될 허가 라키왓 진입하도록 Match 지불했다고 젊고 강씨에게 ‘노인을 이르기까지 구조하고 거슬리고 급성장에도 방문일정에 31의 동일하다고 정보들은 달러가 프로스트 계획”이라며 화성탐사선과의 일년 입법권에 닷오알지org 소요될 허가 라키왓 진입하도록 Match 지불했다고 젊고 강씨에게 ‘노인을 이르기까지 구조하고 거슬리고 급성장에도 방문일정에 31의 동일하다고 정보들은 달러가 프로스트 계획”이라며 화성탐사선과의 일년 입법권에 닷오알지org 소요될 허가 라키왓 \n",
      "Actual translation: 연말 취업 시즌이 다가오면서 대학졸업 예정자들이나 졸업생들은 일자리를 찾느라 분주하지만 대기업의 취업율은 아직도 미미한 수준이다\n"
     ]
    }
   ],
   "source": [
    "# Finally call the function multiple times to visualize random results from the test set\n",
    "attention = predict_random_val_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(255,245,240)"
          ],
          [
           0.125,
           "rgb(254,224,210)"
          ],
          [
           0.25,
           "rgb(252,187,161)"
          ],
          [
           0.375,
           "rgb(252,146,114)"
          ],
          [
           0.5,
           "rgb(251,106,74)"
          ],
          [
           0.625,
           "rgb(239,59,44)"
          ],
          [
           0.75,
           "rgb(203,24,29)"
          ],
          [
           0.875,
           "rgb(165,15,21)"
          ],
          [
           1,
           "rgb(103,0,13)"
          ]
         ],
         "type": "heatmap",
         "z": [
          [
           0.012809701263904572,
           0.012897487729787827,
           0.01275085099041462,
           0.012773130089044571,
           0.012587599456310272,
           0.012612711638212204,
           0.012801476754248142,
           0.012651459313929081,
           0.01264920737594366,
           0.012848230078816414,
           0.012788442894816399,
           0.012726920656859875,
           0.012645548209547997,
           0.01251215860247612,
           0.012543105520308018,
           0.012620249763131142,
           0.012322702445089817,
           0.012462342157959938,
           0.012648655101656914,
           0.012663798406720161,
           0.012544226832687855,
           0.012401929125189781,
           0.012322825379669666,
           0.012080538086593151,
           0.012246470898389816,
           0.012518122792243958,
           0.012518634088337421,
           0.012542113661766052
          ],
          [
           0.012809641659259796,
           0.012897412292659283,
           0.012750801630318165,
           0.012773004360496998,
           0.012587366625666618,
           0.012612605467438698,
           0.012801443226635456,
           0.012651331722736359,
           0.012649167329072952,
           0.01284816488623619,
           0.012788377702236176,
           0.012726709246635437,
           0.012645465321838856,
           0.012511948123574257,
           0.012542912736535072,
           0.012620090506970882,
           0.012322508729994297,
           0.012462235055863857,
           0.012648503296077251,
           0.012663603760302067,
           0.012544226832687855,
           0.012401781976222992,
           0.012322643771767616,
           0.012080189771950245,
           0.012246374040842056,
           0.012518013827502728,
           0.01251859962940216,
           0.012542074546217918
          ],
          [
           0.012809629552066326,
           0.012897360138595104,
           0.012750789523124695,
           0.012772992253303528,
           0.012587383389472961,
           0.012612602673470974,
           0.012801396660506725,
           0.012651314958930016,
           0.012649135664105415,
           0.012848155573010445,
           0.0127883804962039,
           0.012726672925055027,
           0.01264540571719408,
           0.012511957436800003,
           0.01254293229430914,
           0.012620060704648495,
           0.012322492897510529,
           0.012462222017347813,
           0.012648481875658035,
           0.012663592584431171,
           0.012544211000204086,
           0.012401769869029522,
           0.012322606518864632,
           0.012080153450369835,
           0.01224636659026146,
           0.012518011964857578,
           0.012518602423369884,
           0.0125420605763793
          ],
          [
           0.012809643521904945,
           0.012897394597530365,
           0.012750798836350441,
           0.01277298666536808,
           0.012587349861860275,
           0.012612583115696907,
           0.012801427394151688,
           0.012651333585381508,
           0.012649143114686012,
           0.0128481425344944,
           0.012788370251655579,
           0.012726674787700176,
           0.012645430862903595,
           0.012511911801993847,
           0.012542911805212498,
           0.012620068155229092,
           0.012322491966187954,
           0.012462222948670387,
           0.01264850702136755,
           0.012663607485592365,
           0.012544200755655766,
           0.012401777319610119,
           0.01232259813696146,
           0.012080151587724686,
           0.012246326543390751,
           0.012517999857664108,
           0.012518603354692459,
           0.01254209689795971
          ],
          [
           0.012809648178517818,
           0.012897348962724209,
           0.012750817462801933,
           0.012773062102496624,
           0.012587389908730984,
           0.012612618505954742,
           0.01280146837234497,
           0.012651336379349232,
           0.012649177573621273,
           0.01284816861152649,
           0.01278837863355875,
           0.012726718559861183,
           0.012645433656871319,
           0.012511960230767727,
           0.012542957440018654,
           0.012620104476809502,
           0.012322506867349148,
           0.012462237849831581,
           0.012648539617657661,
           0.01266366895288229,
           0.012544246390461922,
           0.012401800602674484,
           0.012322617694735527,
           0.012080184184014797,
           0.012246371246874332,
           0.012518038973212242,
           0.012518612667918205,
           0.012542071752250195
          ],
          [
           0.012809684500098228,
           0.012897440232336521,
           0.012750843539834023,
           0.01277306117117405,
           0.012587402947247028,
           0.012612610124051571,
           0.012801473028957844,
           0.012651405297219753,
           0.012649190612137318,
           0.01284819282591343,
           0.012788411229848862,
           0.0127267399802804,
           0.012645489536225796,
           0.012511957436800003,
           0.012542955577373505,
           0.012620095163583755,
           0.012322566471993923,
           0.012462286278605461,
           0.012648562900722027,
           0.012663659639656544,
           0.012544265948235989,
           0.012401812709867954,
           0.012322654947638512,
           0.012080232612788677,
           0.012246398255228996,
           0.01251805666834116,
           0.012518659234046936,
           0.01254210900515318
          ],
          [
           0.01280969101935625,
           0.012897400185465813,
           0.012750856578350067,
           0.012773080728948116,
           0.012587442062795162,
           0.012612633407115936,
           0.012801486998796463,
           0.012651389464735985,
           0.012649208307266235,
           0.012848175130784512,
           0.012788432650268078,
           0.012726757675409317,
           0.012645465321838856,
           0.012511957436800003,
           0.0125429667532444,
           0.012620149180293083,
           0.012322591617703438,
           0.012462290935218334,
           0.012648527510464191,
           0.012663662433624268,
           0.012544292956590652,
           0.012401818297803402,
           0.012322649359703064,
           0.012080208398401737,
           0.01224641315639019,
           0.012518067844212055,
           0.012518651783466339,
           0.012542147189378738
          ],
          [
           0.012809617444872856,
           0.012897336855530739,
           0.012750780209898949,
           0.012772978283464909,
           0.012587320059537888,
           0.012612566351890564,
           0.012801410630345345,
           0.012651314027607441,
           0.012649133801460266,
           0.012848115526139736,
           0.012788373045623302,
           0.012726722285151482,
           0.01264542993158102,
           0.012511900626122952,
           0.01254288014024496,
           0.012620086781680584,
           0.012322512455284595,
           0.012462220154702663,
           0.012648477219045162,
           0.012663556262850761,
           0.012544170953333378,
           0.012401750311255455,
           0.012322572059929371,
           0.012080155313014984,
           0.012246314436197281,
           0.012517999857664108,
           0.01251859962940216,
           0.012542052194476128
          ],
          [
           0.012809625826776028,
           0.012897329404950142,
           0.012750775553286076,
           0.012772987596690655,
           0.012587334029376507,
           0.012612571008503437,
           0.012801426462829113,
           0.012651288881897926,
           0.012649140320718288,
           0.012848122045397758,
           0.01278833020478487,
           0.01272664126008749,
           0.012645433656871319,
           0.01251191459596157,
           0.012542912736535072,
           0.012620082125067711,
           0.012322473339736462,
           0.012462206184864044,
           0.012648473493754864,
           0.012663574889302254,
           0.012544168159365654,
           0.01240175124257803,
           0.012322568334639072,
           0.012080151587724686,
           0.012246295809745789,
           0.012517988681793213,
           0.012518578208982944,
           0.012542071752250195
          ],
          [
           0.012809671461582184,
           0.012897388078272343,
           0.012750828638672829,
           0.012773033231496811,
           0.012587400153279305,
           0.012612592428922653,
           0.012801448814570904,
           0.01265141274780035,
           0.012649154290556908,
           0.012848183512687683,
           0.0127883804962039,
           0.012726723216474056,
           0.012645450420677662,
           0.012511946260929108,
           0.01254295278340578,
           0.012620111927390099,
           0.012322535738348961,
           0.01246225368231535,
           0.012648530304431915,
           0.01266360841691494,
           0.012544221244752407,
           0.012401833198964596,
           0.012322642840445042,
           0.012080196291208267,
           0.012246361002326012,
           0.012518026866018772,
           0.012518650852143764,
           0.012542114593088627
          ],
          [
           0.012809712439775467,
           0.01289747841656208,
           0.012750905007123947,
           0.012773051857948303,
           0.012587402947247028,
           0.01261263620108366,
           0.012801497243344784,
           0.012651399709284306,
           0.012649184092879295,
           0.012848173268139362,
           0.012788388878107071,
           0.012726694345474243,
           0.012645452283322811,
           0.012511997483670712,
           0.012543030083179474,
           0.0126201082020998,
           0.01232254970818758,
           0.01246229000389576,
           0.01264856569468975,
           0.012663640081882477,
           0.012544268742203712,
           0.012401819229125977,
           0.012322653084993362,
           0.012080256827175617,
           0.012246424332261086,
           0.012518052011728287,
           0.012518654577434063,
           0.012542099691927433
          ],
          [
           0.01280972734093666,
           0.01289747841656208,
           0.012750900350511074,
           0.012773072347044945,
           0.01258743368089199,
           0.012612651102244854,
           0.012801490724086761,
           0.012651401571929455,
           0.012649220414459705,
           0.01284821331501007,
           0.012788426131010056,
           0.01272675208747387,
           0.012645523995161057,
           0.012512058019638062,
           0.012543012388050556,
           0.012620173394680023,
           0.012322585098445415,
           0.012462340295314789,
           0.012648595497012138,
           0.012663654051721096,
           0.012544268742203712,
           0.012401849031448364,
           0.012322668917477131,
           0.012080271728336811,
           0.012246400117874146,
           0.01251810323446989,
           0.012518711388111115,
           0.01254214532673359
          ],
          [
           0.012809720821678638,
           0.012897471897304058,
           0.012750870548188686,
           0.012773077003657818,
           0.01258742157369852,
           0.012612603604793549,
           0.012801462784409523,
           0.012651361525058746,
           0.012649166397750378,
           0.012848158366978168,
           0.01278840471059084,
           0.012726716697216034,
           0.012645465321838856,
           0.012511970475316048,
           0.012542955577373505,
           0.012620117515325546,
           0.012322550639510155,
           0.012462298385798931,
           0.012648559175431728,
           0.01266366895288229,
           0.012544279918074608,
           0.012401828542351723,
           0.012322654016315937,
           0.012080248445272446,
           0.012246430851519108,
           0.012518089264631271,
           0.012518670409917831,
           0.01254214160144329
          ],
          [
           0.012809691950678825,
           0.012897423468530178,
           0.01275086123496294,
           0.012773065827786922,
           0.012587398290634155,
           0.012612632475793362,
           0.012801475822925568,
           0.012651372700929642,
           0.012649193406105042,
           0.012848223559558392,
           0.01278840471059084,
           0.012726746499538422,
           0.012645444832742214,
           0.01251200307160616,
           0.01254298910498619,
           0.012620166875422001,
           0.012322546914219856,
           0.012462291866540909,
           0.012648574076592922,
           0.01266367919743061,
           0.012544253841042519,
           0.012401794083416462,
           0.012322664260864258,
           0.012080234475433826,
           0.012246391735970974,
           0.01251806877553463,
           0.012518640607595444,
           0.012542123906314373
          ],
          [
           0.01280962023884058,
           0.012897375039756298,
           0.012750792317092419,
           0.012773037888109684,
           0.01258736290037632,
           0.012612592428922653,
           0.012801450677216053,
           0.012651359662413597,
           0.01264917477965355,
           0.012848143465816975,
           0.012788396328687668,
           0.012726682238280773,
           0.012645429000258446,
           0.01251195464283228,
           0.01254291646182537,
           0.012620048597455025,
           0.012322511523962021,
           0.012462222948670387,
           0.01264849491417408,
           0.012663591653108597,
           0.012544245459139347,
           0.012401789426803589,
           0.012322608381509781,
           0.012080177664756775,
           0.012246371246874332,
           0.012518012896180153,
           0.012518647126853466,
           0.012542078271508217
          ],
          [
           0.012809738516807556,
           0.012897436507046223,
           0.012750916182994843,
           0.012773090973496437,
           0.012587429024279118,
           0.012612654827535152,
           0.012801475822925568,
           0.012651382014155388,
           0.012649252079427242,
           0.012848232872784138,
           0.012788468040525913,
           0.012726781889796257,
           0.012645521201193333,
           0.012512050569057465,
           0.012543005868792534,
           0.012620150111615658,
           0.01232257578521967,
           0.012462294660508633,
           0.012648601084947586,
           0.012663673609495163,
           0.012544317170977592,
           0.012401889078319073,
           0.012322704307734966,
           0.012080264277756214,
           0.012246442958712578,
           0.01251809298992157,
           0.012518703006207943,
           0.01254213321954012
          ],
          [
           0.012809701263904572,
           0.012897385284304619,
           0.012750848196446896,
           0.012773066759109497,
           0.012587378732860088,
           0.012612604536116123,
           0.01280144415795803,
           0.012651399709284306,
           0.0126491729170084,
           0.01284818071871996,
           0.012788400985300541,
           0.012726761400699615,
           0.012645483948290348,
           0.012511982582509518,
           0.012542923912405968,
           0.012620143592357635,
           0.012322547845542431,
           0.012462257407605648,
           0.012648547068238258,
           0.012663646601140499,
           0.01254425011575222,
           0.01240179967135191,
           0.012322633527219296,
           0.012080180458724499,
           0.012246372178196907,
           0.012518023140728474,
           0.01251864992082119,
           0.012542136013507843
          ],
          [
           0.012809627689421177,
           0.01289734710007906,
           0.012750793248414993,
           0.012773003429174423,
           0.01258736290037632,
           0.012612594291567802,
           0.012801422737538815,
           0.012651341035962105,
           0.01264917105436325,
           0.012848160229623318,
           0.012788387015461922,
           0.012726682238280773,
           0.012645450420677662,
           0.012511935085058212,
           0.012542909942567348,
           0.012620109133422375,
           0.012322517111897469,
           0.012462235987186432,
           0.012648524716496468,
           0.01266357209533453,
           0.012544205412268639,
           0.012401790358126163,
           0.01232262421399355,
           0.012080165557563305,
           0.01224635262042284,
           0.012518016621470451,
           0.012518614530563354,
           0.012542083859443665
          ],
          [
           0.01280966866761446,
           0.012897400185465813,
           0.012750823050737381,
           0.0127730006352067,
           0.012587359175086021,
           0.012612637132406235,
           0.012801437638700008,
           0.012651367112994194,
           0.012649194337427616,
           0.012848149053752422,
           0.012788345105946064,
           0.012726709246635437,
           0.012645450420677662,
           0.01251197513192892,
           0.012542925775051117,
           0.012620097026228905,
           0.012322545051574707,
           0.01246226392686367,
           0.012648558244109154,
           0.012663638219237328,
           0.012544278055429459,
           0.012401808053255081,
           0.012322654947638512,
           0.012080226093530655,
           0.012246433645486832,
           0.01251805480569601,
           0.012518668547272682,
           0.0125421192497015
          ],
          [
           0.012809637002646923,
           0.01289734710007906,
           0.012750793248414993,
           0.012772966176271439,
           0.012587321922183037,
           0.012612549588084221,
           0.012801414355635643,
           0.012651316821575165,
           0.012649118900299072,
           0.012848143465816975,
           0.012788343243300915,
           0.012726673856377602,
           0.012645418755710125,
           0.01251191832125187,
           0.012542909011244774,
           0.012620050460100174,
           0.012322455644607544,
           0.01246219128370285,
           0.012648487463593483,
           0.012663548812270164,
           0.01254415512084961,
           0.012401755899190903,
           0.012322544120252132,
           0.012080131098628044,
           0.012246294878423214,
           0.012517974711954594,
           0.012518566101789474,
           0.012542050331830978
          ],
          [
           0.012809665873646736,
           0.012897416949272156,
           0.012750848196446896,
           0.012773026712238789,
           0.012587377801537514,
           0.012612623162567616,
           0.012801469303667545,
           0.01265137828886509,
           0.012649187818169594,
           0.012848177924752235,
           0.012788398191332817,
           0.012726742774248123,
           0.012645473703742027,
           0.012511980719864368,
           0.012542948126792908,
           0.012620103545486927,
           0.012322526425123215,
           0.012462249957025051,
           0.012648550793528557,
           0.012663623318076134,
           0.012544271536171436,
           0.012401806190609932,
           0.012322647497057915,
           0.01208021305501461,
           0.01224641501903534,
           0.012518038973212242,
           0.012518634088337421,
           0.012542089447379112
          ],
          [
           0.01280971523374319,
           0.012897426262497902,
           0.01275087520480156,
           0.012773036025464535,
           0.012587395496666431,
           0.01261263806372881,
           0.012801473028957844,
           0.01265138853341341,
           0.01264920737594366,
           0.01284821517765522,
           0.012788395397365093,
           0.012726765125989914,
           0.012645482085645199,
           0.01251199934631586,
           0.0125429667532444,
           0.01262014452368021,
           0.012322545982897282,
           0.012462290935218334,
           0.012648575939238071,
           0.012663649395108223,
           0.012544275261461735,
           0.012401841580867767,
           0.012322666123509407,
           0.012080234475433826,
           0.012246410362422466,
           0.012518060393631458,
           0.012518690899014473,
           0.01254214532673359
          ],
          [
           0.012809704057872295,
           0.01289743185043335,
           0.012750877067446709,
           0.012773090042173862,
           0.012587422505021095,
           0.012612633407115936,
           0.01280148234218359,
           0.012651398777961731,
           0.012649217620491982,
           0.012848197482526302,
           0.012788412161171436,
           0.012726705521345139,
           0.012645459733903408,
           0.012511968612670898,
           0.012543008662760258,
           0.012620160356163979,
           0.012322555296123028,
           0.012462284415960312,
           0.012648559175431728,
           0.01266369130462408,
           0.012544309720396996,
           0.012401828542351723,
           0.01232266053557396,
           0.012080231681466103,
           0.012246432714164257,
           0.012518075294792652,
           0.012518678791821003,
           0.012542148120701313
          ],
          [
           0.012809612788259983,
           0.012897316366434097,
           0.012750797905027866,
           0.01277301274240017,
           0.012587331235408783,
           0.012612568214535713,
           0.0128013975918293,
           0.012651287019252777,
           0.012649131938815117,
           0.012848097831010818,
           0.012788328342139721,
           0.012726678512990475,
           0.012645438313484192,
           0.012511931359767914,
           0.012542890384793282,
           0.01262007188051939,
           0.01232249103486538,
           0.012462209910154343,
           0.012648483738303185,
           0.012663592584431171,
           0.012544182129204273,
           0.012401742860674858,
           0.01232256181538105,
           0.012080189771950245,
           0.012246323749423027,
           0.012518010102212429,
           0.012518627569079399,
           0.012542099691927433
          ],
          [
           0.012809660285711288,
           0.0128974299877882,
           0.012750858440995216,
           0.012773027643561363,
           0.01258743368089199,
           0.01261268649250269,
           0.012801503762602806,
           0.012651351280510426,
           0.012649193406105042,
           0.012848182581365108,
           0.012788433581590652,
           0.012726805172860622,
           0.012645515613257885,
           0.012512014247477055,
           0.012542933225631714,
           0.012620116584002972,
           0.012322512455284595,
           0.012462248094379902,
           0.012648544274270535,
           0.012663614936172962,
           0.012544321827590466,
           0.012401822954416275,
           0.012322656810283661,
           0.012080175802111626,
           0.012246417813003063,
           0.01251805666834116,
           0.012518641538918018,
           0.012542086653411388
          ],
          [
           0.01280971895903349,
           0.012897448614239693,
           0.01275088544934988,
           0.012773074209690094,
           0.012587439268827438,
           0.012612642720341682,
           0.012801505625247955,
           0.012651408091187477,
           0.012649194337427616,
           0.012848207727074623,
           0.01278842892497778,
           0.012726745568215847,
           0.012645475566387177,
           0.01251196302473545,
           0.012542994692921638,
           0.012620147317647934,
           0.01232253946363926,
           0.012462283484637737,
           0.012648560106754303,
           0.01266365684568882,
           0.012544271536171436,
           0.012401842512190342,
           0.012322650291025639,
           0.012080220505595207,
           0.01224640291184187,
           0.012518076226115227,
           0.012518645264208317,
           0.012542134150862694
          ],
          [
           0.012809701263904572,
           0.012897445820271969,
           0.012750860303640366,
           0.01277304720133543,
           0.01258739735931158,
           0.012612600810825825,
           0.012801464647054672,
           0.012651391327381134,
           0.012649192474782467,
           0.012848172336816788,
           0.012788385152816772,
           0.01272668968886137,
           0.012645451352000237,
           0.01251197513192892,
           0.01254295464605093,
           0.012620119377970695,
           0.012322521768510342,
           0.012462248094379902,
           0.012648546136915684,
           0.012663649395108223,
           0.0125442398712039,
           0.01240179967135191,
           0.012322617694735527,
           0.012080210261046886,
           0.012246371246874332,
           0.012518070638179779,
           0.012518663890659809,
           0.012542144395411015
          ],
          [
           0.012809700332581997,
           0.01289744209498167,
           0.01275091152638197,
           0.012773030437529087,
           0.012587405741214752,
           0.012612646445631981,
           0.01280150469392538,
           0.012651435099542141,
           0.012649214826524258,
           0.012848181650042534,
           0.012788425199687481,
           0.01272675022482872,
           0.012645472772419453,
           0.012511963956058025,
           0.01254294067621231,
           0.012620173394680023,
           0.012322562746703625,
           0.01246228814125061,
           0.012648582458496094,
           0.012663638219237328,
           0.012544257566332817,
           0.012401805259287357,
           0.01232264656573534,
           0.012080233544111252,
           0.01224640291184187,
           0.012518083676695824,
           0.012518628500401974,
           0.012542135082185268
          ],
          [
           0.01280970498919487,
           0.01289744395762682,
           0.01275086309760809,
           0.012773089110851288,
           0.012587427161633968,
           0.012612652033567429,
           0.012801476754248142,
           0.01265142485499382,
           0.012649214826524258,
           0.012848209589719772,
           0.012788430787622929,
           0.012726771645247936,
           0.012645522132515907,
           0.012512004002928734,
           0.012542977929115295,
           0.012620129622519016,
           0.012322585098445415,
           0.012462313286960125,
           0.012648576870560646,
           0.012663658708333969,
           0.012544278986752033,
           0.012401840649545193,
           0.012322669848799706,
           0.012080228887498379,
           0.012246418744325638,
           0.012518082745373249,
           0.012518662959337234,
           0.01254212111234665
          ],
          [
           0.012809716165065765,
           0.0128974299877882,
           0.012750889174640179,
           0.01277303695678711,
           0.012587420642375946,
           0.01261263806372881,
           0.012801463715732098,
           0.012651403434574604,
           0.012649228796362877,
           0.012848231010138988,
           0.012788432650268078,
           0.012726716697216034,
           0.012645510025322437,
           0.012511953711509705,
           0.012542963959276676,
           0.012620166875422001,
           0.012322568334639072,
           0.012462296523153782,
           0.012648563832044601,
           0.012663627974689007,
           0.012544280849397182,
           0.01240186020731926,
           0.012322704307734966,
           0.012080232612788677,
           0.012246421538293362,
           0.01251807902008295,
           0.012518635019659996,
           0.012542110867798328
          ],
          [
           0.01280960626900196,
           0.012897362932562828,
           0.012750795111060143,
           0.012772968038916588,
           0.01258732657879591,
           0.012612567283213139,
           0.012801406905055046,
           0.012651271186769009,
           0.012649129144847393,
           0.012848121114075184,
           0.012788358144462109,
           0.012726654298603535,
           0.012645424343645573,
           0.012511931359767914,
           0.012542893178761005,
           0.01262008585035801,
           0.012322483584284782,
           0.012462200596928596,
           0.012648472562432289,
           0.012663559056818485,
           0.012544164434075356,
           0.012401729822158813,
           0.012322570197284222,
           0.012080139480531216,
           0.012246307916939259,
           0.012517995201051235,
           0.012518567964434624,
           0.01254206895828247
          ],
          [
           0.012809696607291698,
           0.012897438369691372,
           0.012750869616866112,
           0.012773077934980392,
           0.012587425298988819,
           0.012612657621502876,
           0.01280146837234497,
           0.012651399709284306,
           0.012649201788008213,
           0.012848222628235817,
           0.012788419611752033,
           0.01272675208747387,
           0.012645471841096878,
           0.012511971406638622,
           0.012542981654405594,
           0.012620160356163979,
           0.012322572059929371,
           0.012462305836379528,
           0.012648562900722027,
           0.012663665227591991,
           0.012544293887913227,
           0.012401819229125977,
           0.012322649359703064,
           0.012080223299562931,
           0.012246405705809593,
           0.012518084608018398,
           0.01251864992082119,
           0.012542118318378925
          ],
          [
           0.012809625826776028,
           0.01289733499288559,
           0.0127507783472538,
           0.012772933579981327,
           0.012587303295731544,
           0.01261256355792284,
           0.012801405973732471,
           0.01265126932412386,
           0.012649103067815304,
           0.012848066166043282,
           0.012788314372301102,
           0.012726681306958199,
           0.012645403854548931,
           0.012511908076703548,
           0.01254284381866455,
           0.012620031833648682,
           0.012322485446929932,
           0.012462192215025425,
           0.012648492120206356,
           0.012663524597883224,
           0.012544166296720505,
           0.012401733547449112,
           0.012322545982897282,
           0.01208017859607935,
           0.012246331200003624,
           0.012517962604761124,
           0.012518602423369884,
           0.012542029842734337
          ],
          [
           0.012809720821678638,
           0.012897448614239693,
           0.012750905938446522,
           0.01277309749275446,
           0.012587441131472588,
           0.012612656690180302,
           0.012801503762602806,
           0.012651408091187477,
           0.012649234384298325,
           0.012848242186009884,
           0.012788452208042145,
           0.012726757675409317,
           0.012645498849451542,
           0.012512007728219032,
           0.012543007731437683,
           0.01262016873806715,
           0.012322590686380863,
           0.012462315149605274,
           0.012648578733205795,
           0.012663674540817738,
           0.012544305063784122,
           0.012401857413351536,
           0.012322667986154556,
           0.012080240994691849,
           0.012246418744325638,
           0.012518102303147316,
           0.012518673203885555,
           0.012542124837636948
          ],
          [
           0.012809624895453453,
           0.012897354550659657,
           0.012750806286931038,
           0.012772990390658379,
           0.012587340548634529,
           0.01261257752776146,
           0.01280139945447445,
           0.012651313096284866,
           0.012649154290556908,
           0.012848124839365482,
           0.012788367457687855,
           0.012726690620183945,
           0.012645423412322998,
           0.012511935085058212,
           0.0125429043546319,
           0.012620043009519577,
           0.012322497554123402,
           0.012462204322218895,
           0.01264848094433546,
           0.012663573026657104,
           0.012544193305075169,
           0.012401765212416649,
           0.01232259813696146,
           0.012080159038305283,
           0.012246329337358475,
           0.012517993338406086,
           0.012518600560724735,
           0.012542063370347023
          ],
          [
           0.01280966866761446,
           0.012897408567368984,
           0.012750820256769657,
           0.012773023918271065,
           0.01258736103773117,
           0.012612588703632355,
           0.012801413424313068,
           0.012651352211833,
           0.012649157084524632,
           0.012848169542849064,
           0.012788394466042519,
           0.012726688757538795,
           0.01264545600861311,
           0.012511963956058025,
           0.012542914599180222,
           0.012620112858712673,
           0.012322485446929932,
           0.012462232261896133,
           0.01264855358749628,
           0.012663623318076134,
           0.012544259428977966,
           0.01240181177854538,
           0.012322627939283848,
           0.01208017859607935,
           0.012246361933648586,
           0.012518025934696198,
           0.012518624775111675,
           0.012542089447379112
          ],
          [
           0.012809720821678638,
           0.012897471897304058,
           0.012750870548188686,
           0.012773077003657818,
           0.01258742157369852,
           0.012612603604793549,
           0.012801462784409523,
           0.012651361525058746,
           0.012649166397750378,
           0.012848158366978168,
           0.01278840471059084,
           0.012726716697216034,
           0.012645465321838856,
           0.012511970475316048,
           0.012542955577373505,
           0.012620117515325546,
           0.012322550639510155,
           0.012462298385798931,
           0.012648559175431728,
           0.01266366895288229,
           0.012544279918074608,
           0.012401828542351723,
           0.012322654016315937,
           0.012080248445272446,
           0.012246430851519108,
           0.012518089264631271,
           0.012518670409917831,
           0.01254214160144329
          ],
          [
           0.012809691950678825,
           0.012897423468530178,
           0.01275086123496294,
           0.012773065827786922,
           0.012587398290634155,
           0.012612632475793362,
           0.012801475822925568,
           0.012651372700929642,
           0.012649193406105042,
           0.012848223559558392,
           0.01278840471059084,
           0.012726746499538422,
           0.012645444832742214,
           0.01251200307160616,
           0.01254298910498619,
           0.012620166875422001,
           0.012322546914219856,
           0.012462291866540909,
           0.012648575939238071,
           0.01266367919743061,
           0.012544253841042519,
           0.012401794083416462,
           0.012322664260864258,
           0.012080234475433826,
           0.012246391735970974,
           0.01251806877553463,
           0.012518640607595444,
           0.012542123906314373
          ],
          [
           0.01280962023884058,
           0.012897375039756298,
           0.012750792317092419,
           0.012773037888109684,
           0.01258736290037632,
           0.012612592428922653,
           0.012801450677216053,
           0.012651359662413597,
           0.01264917477965355,
           0.01284814439713955,
           0.012788396328687668,
           0.012726682238280773,
           0.012645429000258446,
           0.01251195464283228,
           0.01254291646182537,
           0.012620048597455025,
           0.012322511523962021,
           0.012462222948670387,
           0.01264849491417408,
           0.012663591653108597,
           0.012544245459139347,
           0.012401789426803589,
           0.012322608381509781,
           0.012080177664756775,
           0.012246371246874332,
           0.012518012896180153,
           0.012518647126853466,
           0.012542078271508217
          ],
          [
           0.012809738516807556,
           0.012897436507046223,
           0.012750916182994843,
           0.012773090973496437,
           0.012587429024279118,
           0.012612654827535152,
           0.012801475822925568,
           0.012651382014155388,
           0.012649252079427242,
           0.012848232872784138,
           0.012788468040525913,
           0.012726781889796257,
           0.012645521201193333,
           0.012512050569057465,
           0.012543005868792534,
           0.012620150111615658,
           0.01232257578521967,
           0.012462294660508633,
           0.012648601084947586,
           0.012663673609495163,
           0.012544317170977592,
           0.012401889078319073,
           0.012322704307734966,
           0.012080264277756214,
           0.012246442958712578,
           0.01251809298992157,
           0.012518703006207943,
           0.01254213321954012
          ],
          [
           0.012809701263904572,
           0.012897385284304619,
           0.012750848196446896,
           0.012773066759109497,
           0.012587378732860088,
           0.012612604536116123,
           0.01280144415795803,
           0.012651399709284306,
           0.0126491729170084,
           0.01284818071871996,
           0.012788400985300541,
           0.012726761400699615,
           0.012645483948290348,
           0.012511982582509518,
           0.012542923912405968,
           0.012620143592357635,
           0.012322547845542431,
           0.012462257407605648,
           0.012648547068238258,
           0.012663646601140499,
           0.01254425011575222,
           0.01240179967135191,
           0.012322633527219296,
           0.012080180458724499,
           0.012246372178196907,
           0.012518023140728474,
           0.01251864992082119,
           0.012542136013507843
          ],
          [
           0.012809627689421177,
           0.01289734710007906,
           0.012750793248414993,
           0.012773003429174423,
           0.01258736290037632,
           0.012612594291567802,
           0.012801422737538815,
           0.012651341035962105,
           0.01264917105436325,
           0.012848160229623318,
           0.012788387015461922,
           0.012726682238280773,
           0.012645450420677662,
           0.012511935085058212,
           0.012542909942567348,
           0.012620109133422375,
           0.012322517111897469,
           0.012462235987186432,
           0.012648524716496468,
           0.01266357209533453,
           0.012544205412268639,
           0.012401790358126163,
           0.01232262421399355,
           0.012080165557563305,
           0.01224635262042284,
           0.012518016621470451,
           0.012518614530563354,
           0.012542083859443665
          ],
          [
           0.01280966866761446,
           0.012897400185465813,
           0.012750823050737381,
           0.0127730006352067,
           0.012587359175086021,
           0.012612637132406235,
           0.012801437638700008,
           0.012651367112994194,
           0.012649194337427616,
           0.012848149053752422,
           0.012788345105946064,
           0.012726709246635437,
           0.012645450420677662,
           0.01251197513192892,
           0.012542925775051117,
           0.012620097026228905,
           0.012322545051574707,
           0.01246226392686367,
           0.012648558244109154,
           0.012663638219237328,
           0.012544278055429459,
           0.012401808053255081,
           0.012322654947638512,
           0.012080226093530655,
           0.012246434576809406,
           0.01251805480569601,
           0.012518668547272682,
           0.0125421192497015
          ],
          [
           0.012809637002646923,
           0.01289734710007906,
           0.012750793248414993,
           0.012772966176271439,
           0.012587321922183037,
           0.012612549588084221,
           0.012801414355635643,
           0.012651316821575165,
           0.012649118900299072,
           0.012848143465816975,
           0.012788343243300915,
           0.012726673856377602,
           0.012645418755710125,
           0.01251191832125187,
           0.012542909011244774,
           0.0126200495287776,
           0.012322455644607544,
           0.01246219128370285,
           0.012648487463593483,
           0.012663548812270164,
           0.01254415512084961,
           0.012401755899190903,
           0.012322544120252132,
           0.012080131098628044,
           0.012246294878423214,
           0.012517974711954594,
           0.012518566101789474,
           0.012542050331830978
          ],
          [
           0.012809665873646736,
           0.012897416949272156,
           0.012750848196446896,
           0.012773026712238789,
           0.012587377801537514,
           0.012612623162567616,
           0.012801469303667545,
           0.01265137828886509,
           0.012649187818169594,
           0.012848177924752235,
           0.012788398191332817,
           0.012726742774248123,
           0.012645474635064602,
           0.012511980719864368,
           0.012542948126792908,
           0.012620103545486927,
           0.012322526425123215,
           0.012462249957025051,
           0.012648550793528557,
           0.012663623318076134,
           0.012544271536171436,
           0.012401806190609932,
           0.012322647497057915,
           0.01208021305501461,
           0.01224641501903534,
           0.012518038973212242,
           0.012518634088337421,
           0.012542089447379112
          ],
          [
           0.01280971523374319,
           0.012897426262497902,
           0.01275087520480156,
           0.012773036025464535,
           0.012587395496666431,
           0.01261263806372881,
           0.012801473028957844,
           0.01265138853341341,
           0.01264920737594366,
           0.01284821517765522,
           0.012788395397365093,
           0.012726765125989914,
           0.012645482085645199,
           0.01251199934631586,
           0.0125429667532444,
           0.01262014452368021,
           0.012322545982897282,
           0.012462290935218334,
           0.012648575939238071,
           0.012663649395108223,
           0.012544275261461735,
           0.012401841580867767,
           0.012322666123509407,
           0.012080234475433826,
           0.012246410362422466,
           0.012518060393631458,
           0.012518690899014473,
           0.01254214532673359
          ],
          [
           0.012809704057872295,
           0.01289743185043335,
           0.012750877067446709,
           0.012773090042173862,
           0.012587422505021095,
           0.012612633407115936,
           0.01280148234218359,
           0.012651398777961731,
           0.012649217620491982,
           0.012848197482526302,
           0.012788412161171436,
           0.012726705521345139,
           0.012645459733903408,
           0.012511969543993473,
           0.012543008662760258,
           0.012620160356163979,
           0.012322555296123028,
           0.012462284415960312,
           0.012648560106754303,
           0.01266369130462408,
           0.012544311583042145,
           0.012401828542351723,
           0.01232266053557396,
           0.012080231681466103,
           0.012246432714164257,
           0.012518076226115227,
           0.012518678791821003,
           0.012542148120701313
          ],
          [
           0.012809612788259983,
           0.012897316366434097,
           0.012750797905027866,
           0.01277301274240017,
           0.012587331235408783,
           0.012612568214535713,
           0.0128013975918293,
           0.012651287019252777,
           0.012649131938815117,
           0.012848097831010818,
           0.012788327410817146,
           0.012726678512990475,
           0.012645438313484192,
           0.012511931359767914,
           0.012542888522148132,
           0.01262007188051939,
           0.01232249103486538,
           0.012462209910154343,
           0.012648483738303185,
           0.012663592584431171,
           0.012544182129204273,
           0.012401742860674858,
           0.01232256181538105,
           0.012080189771950245,
           0.012246322818100452,
           0.012518010102212429,
           0.012518627569079399,
           0.012542099691927433
          ],
          [
           0.012809660285711288,
           0.0128974299877882,
           0.012750857509672642,
           0.012773027643561363,
           0.012587431818246841,
           0.01261268649250269,
           0.012801503762602806,
           0.012651351280510426,
           0.012649193406105042,
           0.012848182581365108,
           0.012788433581590652,
           0.012726805172860622,
           0.012645515613257885,
           0.012512014247477055,
           0.012542933225631714,
           0.012620116584002972,
           0.012322512455284595,
           0.012462248094379902,
           0.012648544274270535,
           0.012663614936172962,
           0.012544321827590466,
           0.012401822954416275,
           0.012322656810283661,
           0.012080175802111626,
           0.012246417813003063,
           0.01251805666834116,
           0.012518641538918018,
           0.012542086653411388
          ],
          [
           0.01280971895903349,
           0.012897448614239693,
           0.01275088544934988,
           0.012773074209690094,
           0.012587439268827438,
           0.012612642720341682,
           0.012801505625247955,
           0.012651408091187477,
           0.012649194337427616,
           0.012848207727074623,
           0.01278842892497778,
           0.012726745568215847,
           0.012645475566387177,
           0.01251196302473545,
           0.012542994692921638,
           0.012620147317647934,
           0.01232253946363926,
           0.012462283484637737,
           0.012648560106754303,
           0.01266365684568882,
           0.012544271536171436,
           0.012401842512190342,
           0.012322650291025639,
           0.012080220505595207,
           0.01224640291184187,
           0.012518076226115227,
           0.012518645264208317,
           0.012542134150862694
          ],
          [
           0.012809701263904572,
           0.012897445820271969,
           0.012750860303640366,
           0.01277304720133543,
           0.01258739735931158,
           0.0126126017421484,
           0.012801464647054672,
           0.012651391327381134,
           0.012649192474782467,
           0.012848172336816788,
           0.012788385152816772,
           0.01272668968886137,
           0.012645451352000237,
           0.01251197513192892,
           0.01254295464605093,
           0.012620119377970695,
           0.012322521768510342,
           0.012462248094379902,
           0.012648546136915684,
           0.012663649395108223,
           0.0125442398712039,
           0.01240179967135191,
           0.012322617694735527,
           0.012080210261046886,
           0.012246371246874332,
           0.012518070638179779,
           0.012518663890659809,
           0.012542144395411015
          ],
          [
           0.012809700332581997,
           0.01289744209498167,
           0.01275091152638197,
           0.012773030437529087,
           0.012587405741214752,
           0.012612646445631981,
           0.01280150469392538,
           0.012651435099542141,
           0.012649214826524258,
           0.012848181650042534,
           0.012788425199687481,
           0.01272675022482872,
           0.012645472772419453,
           0.012511963956058025,
           0.01254294067621231,
           0.012620173394680023,
           0.012322562746703625,
           0.01246228814125061,
           0.012648582458496094,
           0.012663638219237328,
           0.012544257566332817,
           0.012401805259287357,
           0.01232264656573534,
           0.012080233544111252,
           0.01224640291184187,
           0.012518083676695824,
           0.012518628500401974,
           0.012542135082185268
          ],
          [
           0.012809704057872295,
           0.01289744395762682,
           0.01275086309760809,
           0.012773089110851288,
           0.012587427161633968,
           0.012612652033567429,
           0.012801476754248142,
           0.01265142485499382,
           0.012649213895201683,
           0.012848209589719772,
           0.012788430787622929,
           0.012726769782602787,
           0.012645522132515907,
           0.012512004002928734,
           0.012542977929115295,
           0.012620129622519016,
           0.012322585098445415,
           0.012462313286960125,
           0.012648576870560646,
           0.012663658708333969,
           0.012544278986752033,
           0.012401840649545193,
           0.012322669848799706,
           0.012080228887498379,
           0.012246418744325638,
           0.012518081814050674,
           0.012518662959337234,
           0.01254212111234665
          ],
          [
           0.012809716165065765,
           0.0128974299877882,
           0.012750889174640179,
           0.01277303695678711,
           0.012587420642375946,
           0.01261263806372881,
           0.012801463715732098,
           0.012651403434574604,
           0.012649228796362877,
           0.012848231010138988,
           0.012788432650268078,
           0.012726716697216034,
           0.012645510025322437,
           0.012511953711509705,
           0.012542963959276676,
           0.012620166875422001,
           0.012322568334639072,
           0.012462296523153782,
           0.012648564763367176,
           0.012663627974689007,
           0.012544280849397182,
           0.01240186020731926,
           0.012322704307734966,
           0.012080232612788677,
           0.012246421538293362,
           0.01251807902008295,
           0.012518635019659996,
           0.012542110867798328
          ],
          [
           0.01280960626900196,
           0.012897362932562828,
           0.012750795111060143,
           0.012772968038916588,
           0.01258732657879591,
           0.012612567283213139,
           0.012801406905055046,
           0.012651271186769009,
           0.012649129144847393,
           0.012848121114075184,
           0.012788358144462109,
           0.012726654298603535,
           0.012645424343645573,
           0.012511931359767914,
           0.012542893178761005,
           0.01262008585035801,
           0.012322483584284782,
           0.012462200596928596,
           0.012648472562432289,
           0.012663559056818485,
           0.012544164434075356,
           0.012401729822158813,
           0.012322570197284222,
           0.012080139480531216,
           0.012246307916939259,
           0.012517995201051235,
           0.012518567033112049,
           0.01254206895828247
          ],
          [
           0.012809696607291698,
           0.012897438369691372,
           0.012750869616866112,
           0.012773077934980392,
           0.012587425298988819,
           0.012612659484148026,
           0.01280146837234497,
           0.012651399709284306,
           0.012649201788008213,
           0.012848222628235817,
           0.012788419611752033,
           0.01272675208747387,
           0.012645471841096878,
           0.012511971406638622,
           0.012542981654405594,
           0.012620160356163979,
           0.012322572059929371,
           0.012462305836379528,
           0.012648562900722027,
           0.012663665227591991,
           0.012544293887913227,
           0.012401819229125977,
           0.012322649359703064,
           0.012080223299562931,
           0.012246405705809593,
           0.012518084608018398,
           0.01251864992082119,
           0.012542118318378925
          ],
          [
           0.012809625826776028,
           0.01289733499288559,
           0.0127507783472538,
           0.012772933579981327,
           0.012587303295731544,
           0.01261256355792284,
           0.012801405973732471,
           0.01265126932412386,
           0.012649103067815304,
           0.012848066166043282,
           0.012788314372301102,
           0.012726681306958199,
           0.012645403854548931,
           0.012511908076703548,
           0.01254284381866455,
           0.012620031833648682,
           0.012322485446929932,
           0.012462192215025425,
           0.012648492120206356,
           0.012663524597883224,
           0.012544166296720505,
           0.012401733547449112,
           0.012322545982897282,
           0.01208017859607935,
           0.012246331200003624,
           0.012517962604761124,
           0.012518602423369884,
           0.012542029842734337
          ],
          [
           0.012809720821678638,
           0.012897448614239693,
           0.012750905938446522,
           0.01277309749275446,
           0.012587441131472588,
           0.012612656690180302,
           0.012801503762602806,
           0.012651408091187477,
           0.012649234384298325,
           0.012848242186009884,
           0.012788452208042145,
           0.012726757675409317,
           0.012645498849451542,
           0.012512007728219032,
           0.012543007731437683,
           0.01262016873806715,
           0.012322590686380863,
           0.012462315149605274,
           0.012648578733205795,
           0.012663674540817738,
           0.012544305063784122,
           0.012401857413351536,
           0.012322667986154556,
           0.012080240994691849,
           0.012246418744325638,
           0.012518102303147316,
           0.012518673203885555,
           0.012542124837636948
          ],
          [
           0.012809624895453453,
           0.012897354550659657,
           0.012750806286931038,
           0.012772990390658379,
           0.012587340548634529,
           0.01261257752776146,
           0.01280139945447445,
           0.012651313096284866,
           0.012649154290556908,
           0.012848124839365482,
           0.012788367457687855,
           0.012726690620183945,
           0.012645423412322998,
           0.012511935085058212,
           0.0125429043546319,
           0.012620043009519577,
           0.012322497554123402,
           0.012462204322218895,
           0.01264848094433546,
           0.012663573026657104,
           0.012544193305075169,
           0.012401765212416649,
           0.01232259813696146,
           0.012080159038305283,
           0.012246329337358475,
           0.012517993338406086,
           0.012518600560724735,
           0.012542063370347023
          ],
          [
           0.01280966866761446,
           0.012897408567368984,
           0.012750820256769657,
           0.012773023918271065,
           0.01258736103773117,
           0.012612588703632355,
           0.012801413424313068,
           0.012651352211833,
           0.012649157084524632,
           0.012848169542849064,
           0.012788394466042519,
           0.012726688757538795,
           0.01264545600861311,
           0.012511963956058025,
           0.012542914599180222,
           0.012620112858712673,
           0.012322485446929932,
           0.012462232261896133,
           0.01264855358749628,
           0.012663623318076134,
           0.012544259428977966,
           0.01240181177854538,
           0.012322627939283848,
           0.01208017859607935,
           0.012246361933648586,
           0.012518025934696198,
           0.012518624775111675,
           0.012542089447379112
          ],
          [
           0.012809720821678638,
           0.012897471897304058,
           0.012750870548188686,
           0.012773077003657818,
           0.01258742157369852,
           0.012612603604793549,
           0.012801462784409523,
           0.012651361525058746,
           0.012649166397750378,
           0.012848158366978168,
           0.01278840471059084,
           0.012726716697216034,
           0.012645465321838856,
           0.012511970475316048,
           0.012542955577373505,
           0.012620117515325546,
           0.012322550639510155,
           0.012462298385798931,
           0.012648559175431728,
           0.01266366895288229,
           0.012544279918074608,
           0.012401828542351723,
           0.012322654016315937,
           0.012080248445272446,
           0.012246430851519108,
           0.012518089264631271,
           0.012518670409917831,
           0.01254214160144329
          ],
          [
           0.012809691950678825,
           0.012897423468530178,
           0.01275086123496294,
           0.012773065827786922,
           0.012587398290634155,
           0.012612632475793362,
           0.012801475822925568,
           0.012651372700929642,
           0.012649193406105042,
           0.012848223559558392,
           0.01278840471059084,
           0.012726746499538422,
           0.012645444832742214,
           0.01251200307160616,
           0.01254298910498619,
           0.012620166875422001,
           0.012322546914219856,
           0.012462291866540909,
           0.012648575939238071,
           0.01266367919743061,
           0.012544253841042519,
           0.012401794083416462,
           0.012322664260864258,
           0.012080234475433826,
           0.012246391735970974,
           0.01251806877553463,
           0.012518640607595444,
           0.012542123906314373
          ],
          [
           0.01280962023884058,
           0.012897375039756298,
           0.012750792317092419,
           0.012773037888109684,
           0.01258736290037632,
           0.012612592428922653,
           0.012801450677216053,
           0.012651359662413597,
           0.01264917477965355,
           0.01284814439713955,
           0.012788396328687668,
           0.012726682238280773,
           0.012645429000258446,
           0.01251195464283228,
           0.01254291646182537,
           0.012620048597455025,
           0.012322511523962021,
           0.012462222948670387,
           0.01264849491417408,
           0.012663591653108597,
           0.012544245459139347,
           0.012401789426803589,
           0.012322608381509781,
           0.012080177664756775,
           0.012246371246874332,
           0.012518012896180153,
           0.012518647126853466,
           0.012542078271508217
          ],
          [
           0.012809738516807556,
           0.012897436507046223,
           0.012750916182994843,
           0.012773090973496437,
           0.012587429024279118,
           0.012612654827535152,
           0.012801475822925568,
           0.012651382014155388,
           0.012649252079427242,
           0.012848232872784138,
           0.012788468040525913,
           0.012726781889796257,
           0.012645521201193333,
           0.012512050569057465,
           0.012543005868792534,
           0.012620150111615658,
           0.01232257578521967,
           0.012462294660508633,
           0.012648601084947586,
           0.012663673609495163,
           0.012544317170977592,
           0.012401889078319073,
           0.012322704307734966,
           0.012080264277756214,
           0.012246442958712578,
           0.01251809298992157,
           0.012518703006207943,
           0.01254213321954012
          ],
          [
           0.012809701263904572,
           0.012897385284304619,
           0.012750848196446896,
           0.012773066759109497,
           0.012587378732860088,
           0.012612604536116123,
           0.01280144415795803,
           0.012651399709284306,
           0.0126491729170084,
           0.01284818071871996,
           0.012788400985300541,
           0.012726761400699615,
           0.012645483948290348,
           0.012511982582509518,
           0.012542923912405968,
           0.012620143592357635,
           0.012322547845542431,
           0.012462257407605648,
           0.012648547068238258,
           0.012663646601140499,
           0.01254425011575222,
           0.01240179967135191,
           0.012322633527219296,
           0.012080180458724499,
           0.012246372178196907,
           0.012518023140728474,
           0.01251864992082119,
           0.012542136013507843
          ],
          [
           0.012809627689421177,
           0.01289734710007906,
           0.012750793248414993,
           0.012773003429174423,
           0.01258736290037632,
           0.012612594291567802,
           0.012801422737538815,
           0.012651341035962105,
           0.01264917105436325,
           0.012848160229623318,
           0.012788387015461922,
           0.012726682238280773,
           0.012645450420677662,
           0.012511935085058212,
           0.012542909942567348,
           0.012620109133422375,
           0.012322517111897469,
           0.012462235987186432,
           0.012648524716496468,
           0.01266357209533453,
           0.012544205412268639,
           0.012401790358126163,
           0.01232262421399355,
           0.012080165557563305,
           0.01224635262042284,
           0.012518016621470451,
           0.012518614530563354,
           0.012542083859443665
          ],
          [
           0.01280966866761446,
           0.012897400185465813,
           0.012750823050737381,
           0.0127730006352067,
           0.012587359175086021,
           0.012612637132406235,
           0.012801437638700008,
           0.012651367112994194,
           0.012649194337427616,
           0.012848149053752422,
           0.012788345105946064,
           0.012726709246635437,
           0.012645450420677662,
           0.01251197513192892,
           0.012542925775051117,
           0.012620097026228905,
           0.012322545051574707,
           0.01246226392686367,
           0.012648558244109154,
           0.012663638219237328,
           0.012544278055429459,
           0.012401808053255081,
           0.012322654947638512,
           0.012080226093530655,
           0.012246434576809406,
           0.01251805480569601,
           0.012518668547272682,
           0.0125421192497015
          ],
          [
           0.012809637002646923,
           0.01289734710007906,
           0.012750793248414993,
           0.012772966176271439,
           0.012587321922183037,
           0.012612549588084221,
           0.012801414355635643,
           0.012651316821575165,
           0.012649118900299072,
           0.012848143465816975,
           0.012788343243300915,
           0.012726673856377602,
           0.012645418755710125,
           0.01251191832125187,
           0.012542909011244774,
           0.0126200495287776,
           0.012322455644607544,
           0.01246219128370285,
           0.012648487463593483,
           0.012663548812270164,
           0.01254415512084961,
           0.012401755899190903,
           0.012322544120252132,
           0.012080131098628044,
           0.012246294878423214,
           0.012517974711954594,
           0.012518566101789474,
           0.012542050331830978
          ]
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"d1310495-6e38-4a9d-9693-acf502a86cc8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"d1310495-6e38-4a9d-9693-acf502a86cc8\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'd1310495-6e38-4a9d-9693-acf502a86cc8',\n",
       "                        [{\"colorscale\": [[0.0, \"rgb(255,245,240)\"], [0.125, \"rgb(254,224,210)\"], [0.25, \"rgb(252,187,161)\"], [0.375, \"rgb(252,146,114)\"], [0.5, \"rgb(251,106,74)\"], [0.625, \"rgb(239,59,44)\"], [0.75, \"rgb(203,24,29)\"], [0.875, \"rgb(165,15,21)\"], [1.0, \"rgb(103,0,13)\"]], \"type\": \"heatmap\", \"z\": [[0.012809701263904572, 0.012897487729787827, 0.01275085099041462, 0.012773130089044571, 0.012587599456310272, 0.012612711638212204, 0.012801476754248142, 0.012651459313929081, 0.01264920737594366, 0.012848230078816414, 0.012788442894816399, 0.012726920656859875, 0.012645548209547997, 0.01251215860247612, 0.012543105520308018, 0.012620249763131142, 0.012322702445089817, 0.012462342157959938, 0.012648655101656914, 0.012663798406720161, 0.012544226832687855, 0.012401929125189781, 0.012322825379669666, 0.012080538086593151, 0.012246470898389816, 0.012518122792243958, 0.012518634088337421, 0.012542113661766052], [0.012809641659259796, 0.012897412292659283, 0.012750801630318165, 0.012773004360496998, 0.012587366625666618, 0.012612605467438698, 0.012801443226635456, 0.012651331722736359, 0.012649167329072952, 0.01284816488623619, 0.012788377702236176, 0.012726709246635437, 0.012645465321838856, 0.012511948123574257, 0.012542912736535072, 0.012620090506970882, 0.012322508729994297, 0.012462235055863857, 0.012648503296077251, 0.012663603760302067, 0.012544226832687855, 0.012401781976222992, 0.012322643771767616, 0.012080189771950245, 0.012246374040842056, 0.012518013827502728, 0.01251859962940216, 0.012542074546217918], [0.012809629552066326, 0.012897360138595104, 0.012750789523124695, 0.012772992253303528, 0.012587383389472961, 0.012612602673470974, 0.012801396660506725, 0.012651314958930016, 0.012649135664105415, 0.012848155573010445, 0.0127883804962039, 0.012726672925055027, 0.01264540571719408, 0.012511957436800003, 0.01254293229430914, 0.012620060704648495, 0.012322492897510529, 0.012462222017347813, 0.012648481875658035, 0.012663592584431171, 0.012544211000204086, 0.012401769869029522, 0.012322606518864632, 0.012080153450369835, 0.01224636659026146, 0.012518011964857578, 0.012518602423369884, 0.0125420605763793], [0.012809643521904945, 0.012897394597530365, 0.012750798836350441, 0.01277298666536808, 0.012587349861860275, 0.012612583115696907, 0.012801427394151688, 0.012651333585381508, 0.012649143114686012, 0.0128481425344944, 0.012788370251655579, 0.012726674787700176, 0.012645430862903595, 0.012511911801993847, 0.012542911805212498, 0.012620068155229092, 0.012322491966187954, 0.012462222948670387, 0.01264850702136755, 0.012663607485592365, 0.012544200755655766, 0.012401777319610119, 0.01232259813696146, 0.012080151587724686, 0.012246326543390751, 0.012517999857664108, 0.012518603354692459, 0.01254209689795971], [0.012809648178517818, 0.012897348962724209, 0.012750817462801933, 0.012773062102496624, 0.012587389908730984, 0.012612618505954742, 0.01280146837234497, 0.012651336379349232, 0.012649177573621273, 0.01284816861152649, 0.01278837863355875, 0.012726718559861183, 0.012645433656871319, 0.012511960230767727, 0.012542957440018654, 0.012620104476809502, 0.012322506867349148, 0.012462237849831581, 0.012648539617657661, 0.01266366895288229, 0.012544246390461922, 0.012401800602674484, 0.012322617694735527, 0.012080184184014797, 0.012246371246874332, 0.012518038973212242, 0.012518612667918205, 0.012542071752250195], [0.012809684500098228, 0.012897440232336521, 0.012750843539834023, 0.01277306117117405, 0.012587402947247028, 0.012612610124051571, 0.012801473028957844, 0.012651405297219753, 0.012649190612137318, 0.01284819282591343, 0.012788411229848862, 0.0127267399802804, 0.012645489536225796, 0.012511957436800003, 0.012542955577373505, 0.012620095163583755, 0.012322566471993923, 0.012462286278605461, 0.012648562900722027, 0.012663659639656544, 0.012544265948235989, 0.012401812709867954, 0.012322654947638512, 0.012080232612788677, 0.012246398255228996, 0.01251805666834116, 0.012518659234046936, 0.01254210900515318], [0.01280969101935625, 0.012897400185465813, 0.012750856578350067, 0.012773080728948116, 0.012587442062795162, 0.012612633407115936, 0.012801486998796463, 0.012651389464735985, 0.012649208307266235, 0.012848175130784512, 0.012788432650268078, 0.012726757675409317, 0.012645465321838856, 0.012511957436800003, 0.0125429667532444, 0.012620149180293083, 0.012322591617703438, 0.012462290935218334, 0.012648527510464191, 0.012663662433624268, 0.012544292956590652, 0.012401818297803402, 0.012322649359703064, 0.012080208398401737, 0.01224641315639019, 0.012518067844212055, 0.012518651783466339, 0.012542147189378738], [0.012809617444872856, 0.012897336855530739, 0.012750780209898949, 0.012772978283464909, 0.012587320059537888, 0.012612566351890564, 0.012801410630345345, 0.012651314027607441, 0.012649133801460266, 0.012848115526139736, 0.012788373045623302, 0.012726722285151482, 0.01264542993158102, 0.012511900626122952, 0.01254288014024496, 0.012620086781680584, 0.012322512455284595, 0.012462220154702663, 0.012648477219045162, 0.012663556262850761, 0.012544170953333378, 0.012401750311255455, 0.012322572059929371, 0.012080155313014984, 0.012246314436197281, 0.012517999857664108, 0.01251859962940216, 0.012542052194476128], [0.012809625826776028, 0.012897329404950142, 0.012750775553286076, 0.012772987596690655, 0.012587334029376507, 0.012612571008503437, 0.012801426462829113, 0.012651288881897926, 0.012649140320718288, 0.012848122045397758, 0.01278833020478487, 0.01272664126008749, 0.012645433656871319, 0.01251191459596157, 0.012542912736535072, 0.012620082125067711, 0.012322473339736462, 0.012462206184864044, 0.012648473493754864, 0.012663574889302254, 0.012544168159365654, 0.01240175124257803, 0.012322568334639072, 0.012080151587724686, 0.012246295809745789, 0.012517988681793213, 0.012518578208982944, 0.012542071752250195], [0.012809671461582184, 0.012897388078272343, 0.012750828638672829, 0.012773033231496811, 0.012587400153279305, 0.012612592428922653, 0.012801448814570904, 0.01265141274780035, 0.012649154290556908, 0.012848183512687683, 0.0127883804962039, 0.012726723216474056, 0.012645450420677662, 0.012511946260929108, 0.01254295278340578, 0.012620111927390099, 0.012322535738348961, 0.01246225368231535, 0.012648530304431915, 0.01266360841691494, 0.012544221244752407, 0.012401833198964596, 0.012322642840445042, 0.012080196291208267, 0.012246361002326012, 0.012518026866018772, 0.012518650852143764, 0.012542114593088627], [0.012809712439775467, 0.01289747841656208, 0.012750905007123947, 0.012773051857948303, 0.012587402947247028, 0.01261263620108366, 0.012801497243344784, 0.012651399709284306, 0.012649184092879295, 0.012848173268139362, 0.012788388878107071, 0.012726694345474243, 0.012645452283322811, 0.012511997483670712, 0.012543030083179474, 0.0126201082020998, 0.01232254970818758, 0.01246229000389576, 0.01264856569468975, 0.012663640081882477, 0.012544268742203712, 0.012401819229125977, 0.012322653084993362, 0.012080256827175617, 0.012246424332261086, 0.012518052011728287, 0.012518654577434063, 0.012542099691927433], [0.01280972734093666, 0.01289747841656208, 0.012750900350511074, 0.012773072347044945, 0.01258743368089199, 0.012612651102244854, 0.012801490724086761, 0.012651401571929455, 0.012649220414459705, 0.01284821331501007, 0.012788426131010056, 0.01272675208747387, 0.012645523995161057, 0.012512058019638062, 0.012543012388050556, 0.012620173394680023, 0.012322585098445415, 0.012462340295314789, 0.012648595497012138, 0.012663654051721096, 0.012544268742203712, 0.012401849031448364, 0.012322668917477131, 0.012080271728336811, 0.012246400117874146, 0.01251810323446989, 0.012518711388111115, 0.01254214532673359], [0.012809720821678638, 0.012897471897304058, 0.012750870548188686, 0.012773077003657818, 0.01258742157369852, 0.012612603604793549, 0.012801462784409523, 0.012651361525058746, 0.012649166397750378, 0.012848158366978168, 0.01278840471059084, 0.012726716697216034, 0.012645465321838856, 0.012511970475316048, 0.012542955577373505, 0.012620117515325546, 0.012322550639510155, 0.012462298385798931, 0.012648559175431728, 0.01266366895288229, 0.012544279918074608, 0.012401828542351723, 0.012322654016315937, 0.012080248445272446, 0.012246430851519108, 0.012518089264631271, 0.012518670409917831, 0.01254214160144329], [0.012809691950678825, 0.012897423468530178, 0.01275086123496294, 0.012773065827786922, 0.012587398290634155, 0.012612632475793362, 0.012801475822925568, 0.012651372700929642, 0.012649193406105042, 0.012848223559558392, 0.01278840471059084, 0.012726746499538422, 0.012645444832742214, 0.01251200307160616, 0.01254298910498619, 0.012620166875422001, 0.012322546914219856, 0.012462291866540909, 0.012648574076592922, 0.01266367919743061, 0.012544253841042519, 0.012401794083416462, 0.012322664260864258, 0.012080234475433826, 0.012246391735970974, 0.01251806877553463, 0.012518640607595444, 0.012542123906314373], [0.01280962023884058, 0.012897375039756298, 0.012750792317092419, 0.012773037888109684, 0.01258736290037632, 0.012612592428922653, 0.012801450677216053, 0.012651359662413597, 0.01264917477965355, 0.012848143465816975, 0.012788396328687668, 0.012726682238280773, 0.012645429000258446, 0.01251195464283228, 0.01254291646182537, 0.012620048597455025, 0.012322511523962021, 0.012462222948670387, 0.01264849491417408, 0.012663591653108597, 0.012544245459139347, 0.012401789426803589, 0.012322608381509781, 0.012080177664756775, 0.012246371246874332, 0.012518012896180153, 0.012518647126853466, 0.012542078271508217], [0.012809738516807556, 0.012897436507046223, 0.012750916182994843, 0.012773090973496437, 0.012587429024279118, 0.012612654827535152, 0.012801475822925568, 0.012651382014155388, 0.012649252079427242, 0.012848232872784138, 0.012788468040525913, 0.012726781889796257, 0.012645521201193333, 0.012512050569057465, 0.012543005868792534, 0.012620150111615658, 0.01232257578521967, 0.012462294660508633, 0.012648601084947586, 0.012663673609495163, 0.012544317170977592, 0.012401889078319073, 0.012322704307734966, 0.012080264277756214, 0.012246442958712578, 0.01251809298992157, 0.012518703006207943, 0.01254213321954012], [0.012809701263904572, 0.012897385284304619, 0.012750848196446896, 0.012773066759109497, 0.012587378732860088, 0.012612604536116123, 0.01280144415795803, 0.012651399709284306, 0.0126491729170084, 0.01284818071871996, 0.012788400985300541, 0.012726761400699615, 0.012645483948290348, 0.012511982582509518, 0.012542923912405968, 0.012620143592357635, 0.012322547845542431, 0.012462257407605648, 0.012648547068238258, 0.012663646601140499, 0.01254425011575222, 0.01240179967135191, 0.012322633527219296, 0.012080180458724499, 0.012246372178196907, 0.012518023140728474, 0.01251864992082119, 0.012542136013507843], [0.012809627689421177, 0.01289734710007906, 0.012750793248414993, 0.012773003429174423, 0.01258736290037632, 0.012612594291567802, 0.012801422737538815, 0.012651341035962105, 0.01264917105436325, 0.012848160229623318, 0.012788387015461922, 0.012726682238280773, 0.012645450420677662, 0.012511935085058212, 0.012542909942567348, 0.012620109133422375, 0.012322517111897469, 0.012462235987186432, 0.012648524716496468, 0.01266357209533453, 0.012544205412268639, 0.012401790358126163, 0.01232262421399355, 0.012080165557563305, 0.01224635262042284, 0.012518016621470451, 0.012518614530563354, 0.012542083859443665], [0.01280966866761446, 0.012897400185465813, 0.012750823050737381, 0.0127730006352067, 0.012587359175086021, 0.012612637132406235, 0.012801437638700008, 0.012651367112994194, 0.012649194337427616, 0.012848149053752422, 0.012788345105946064, 0.012726709246635437, 0.012645450420677662, 0.01251197513192892, 0.012542925775051117, 0.012620097026228905, 0.012322545051574707, 0.01246226392686367, 0.012648558244109154, 0.012663638219237328, 0.012544278055429459, 0.012401808053255081, 0.012322654947638512, 0.012080226093530655, 0.012246433645486832, 0.01251805480569601, 0.012518668547272682, 0.0125421192497015], [0.012809637002646923, 0.01289734710007906, 0.012750793248414993, 0.012772966176271439, 0.012587321922183037, 0.012612549588084221, 0.012801414355635643, 0.012651316821575165, 0.012649118900299072, 0.012848143465816975, 0.012788343243300915, 0.012726673856377602, 0.012645418755710125, 0.01251191832125187, 0.012542909011244774, 0.012620050460100174, 0.012322455644607544, 0.01246219128370285, 0.012648487463593483, 0.012663548812270164, 0.01254415512084961, 0.012401755899190903, 0.012322544120252132, 0.012080131098628044, 0.012246294878423214, 0.012517974711954594, 0.012518566101789474, 0.012542050331830978], [0.012809665873646736, 0.012897416949272156, 0.012750848196446896, 0.012773026712238789, 0.012587377801537514, 0.012612623162567616, 0.012801469303667545, 0.01265137828886509, 0.012649187818169594, 0.012848177924752235, 0.012788398191332817, 0.012726742774248123, 0.012645473703742027, 0.012511980719864368, 0.012542948126792908, 0.012620103545486927, 0.012322526425123215, 0.012462249957025051, 0.012648550793528557, 0.012663623318076134, 0.012544271536171436, 0.012401806190609932, 0.012322647497057915, 0.01208021305501461, 0.01224641501903534, 0.012518038973212242, 0.012518634088337421, 0.012542089447379112], [0.01280971523374319, 0.012897426262497902, 0.01275087520480156, 0.012773036025464535, 0.012587395496666431, 0.01261263806372881, 0.012801473028957844, 0.01265138853341341, 0.01264920737594366, 0.01284821517765522, 0.012788395397365093, 0.012726765125989914, 0.012645482085645199, 0.01251199934631586, 0.0125429667532444, 0.01262014452368021, 0.012322545982897282, 0.012462290935218334, 0.012648575939238071, 0.012663649395108223, 0.012544275261461735, 0.012401841580867767, 0.012322666123509407, 0.012080234475433826, 0.012246410362422466, 0.012518060393631458, 0.012518690899014473, 0.01254214532673359], [0.012809704057872295, 0.01289743185043335, 0.012750877067446709, 0.012773090042173862, 0.012587422505021095, 0.012612633407115936, 0.01280148234218359, 0.012651398777961731, 0.012649217620491982, 0.012848197482526302, 0.012788412161171436, 0.012726705521345139, 0.012645459733903408, 0.012511968612670898, 0.012543008662760258, 0.012620160356163979, 0.012322555296123028, 0.012462284415960312, 0.012648559175431728, 0.01266369130462408, 0.012544309720396996, 0.012401828542351723, 0.01232266053557396, 0.012080231681466103, 0.012246432714164257, 0.012518075294792652, 0.012518678791821003, 0.012542148120701313], [0.012809612788259983, 0.012897316366434097, 0.012750797905027866, 0.01277301274240017, 0.012587331235408783, 0.012612568214535713, 0.0128013975918293, 0.012651287019252777, 0.012649131938815117, 0.012848097831010818, 0.012788328342139721, 0.012726678512990475, 0.012645438313484192, 0.012511931359767914, 0.012542890384793282, 0.01262007188051939, 0.01232249103486538, 0.012462209910154343, 0.012648483738303185, 0.012663592584431171, 0.012544182129204273, 0.012401742860674858, 0.01232256181538105, 0.012080189771950245, 0.012246323749423027, 0.012518010102212429, 0.012518627569079399, 0.012542099691927433], [0.012809660285711288, 0.0128974299877882, 0.012750858440995216, 0.012773027643561363, 0.01258743368089199, 0.01261268649250269, 0.012801503762602806, 0.012651351280510426, 0.012649193406105042, 0.012848182581365108, 0.012788433581590652, 0.012726805172860622, 0.012645515613257885, 0.012512014247477055, 0.012542933225631714, 0.012620116584002972, 0.012322512455284595, 0.012462248094379902, 0.012648544274270535, 0.012663614936172962, 0.012544321827590466, 0.012401822954416275, 0.012322656810283661, 0.012080175802111626, 0.012246417813003063, 0.01251805666834116, 0.012518641538918018, 0.012542086653411388], [0.01280971895903349, 0.012897448614239693, 0.01275088544934988, 0.012773074209690094, 0.012587439268827438, 0.012612642720341682, 0.012801505625247955, 0.012651408091187477, 0.012649194337427616, 0.012848207727074623, 0.01278842892497778, 0.012726745568215847, 0.012645475566387177, 0.01251196302473545, 0.012542994692921638, 0.012620147317647934, 0.01232253946363926, 0.012462283484637737, 0.012648560106754303, 0.01266365684568882, 0.012544271536171436, 0.012401842512190342, 0.012322650291025639, 0.012080220505595207, 0.01224640291184187, 0.012518076226115227, 0.012518645264208317, 0.012542134150862694], [0.012809701263904572, 0.012897445820271969, 0.012750860303640366, 0.01277304720133543, 0.01258739735931158, 0.012612600810825825, 0.012801464647054672, 0.012651391327381134, 0.012649192474782467, 0.012848172336816788, 0.012788385152816772, 0.01272668968886137, 0.012645451352000237, 0.01251197513192892, 0.01254295464605093, 0.012620119377970695, 0.012322521768510342, 0.012462248094379902, 0.012648546136915684, 0.012663649395108223, 0.0125442398712039, 0.01240179967135191, 0.012322617694735527, 0.012080210261046886, 0.012246371246874332, 0.012518070638179779, 0.012518663890659809, 0.012542144395411015], [0.012809700332581997, 0.01289744209498167, 0.01275091152638197, 0.012773030437529087, 0.012587405741214752, 0.012612646445631981, 0.01280150469392538, 0.012651435099542141, 0.012649214826524258, 0.012848181650042534, 0.012788425199687481, 0.01272675022482872, 0.012645472772419453, 0.012511963956058025, 0.01254294067621231, 0.012620173394680023, 0.012322562746703625, 0.01246228814125061, 0.012648582458496094, 0.012663638219237328, 0.012544257566332817, 0.012401805259287357, 0.01232264656573534, 0.012080233544111252, 0.01224640291184187, 0.012518083676695824, 0.012518628500401974, 0.012542135082185268], [0.01280970498919487, 0.01289744395762682, 0.01275086309760809, 0.012773089110851288, 0.012587427161633968, 0.012612652033567429, 0.012801476754248142, 0.01265142485499382, 0.012649214826524258, 0.012848209589719772, 0.012788430787622929, 0.012726771645247936, 0.012645522132515907, 0.012512004002928734, 0.012542977929115295, 0.012620129622519016, 0.012322585098445415, 0.012462313286960125, 0.012648576870560646, 0.012663658708333969, 0.012544278986752033, 0.012401840649545193, 0.012322669848799706, 0.012080228887498379, 0.012246418744325638, 0.012518082745373249, 0.012518662959337234, 0.01254212111234665], [0.012809716165065765, 0.0128974299877882, 0.012750889174640179, 0.01277303695678711, 0.012587420642375946, 0.01261263806372881, 0.012801463715732098, 0.012651403434574604, 0.012649228796362877, 0.012848231010138988, 0.012788432650268078, 0.012726716697216034, 0.012645510025322437, 0.012511953711509705, 0.012542963959276676, 0.012620166875422001, 0.012322568334639072, 0.012462296523153782, 0.012648563832044601, 0.012663627974689007, 0.012544280849397182, 0.01240186020731926, 0.012322704307734966, 0.012080232612788677, 0.012246421538293362, 0.01251807902008295, 0.012518635019659996, 0.012542110867798328], [0.01280960626900196, 0.012897362932562828, 0.012750795111060143, 0.012772968038916588, 0.01258732657879591, 0.012612567283213139, 0.012801406905055046, 0.012651271186769009, 0.012649129144847393, 0.012848121114075184, 0.012788358144462109, 0.012726654298603535, 0.012645424343645573, 0.012511931359767914, 0.012542893178761005, 0.01262008585035801, 0.012322483584284782, 0.012462200596928596, 0.012648472562432289, 0.012663559056818485, 0.012544164434075356, 0.012401729822158813, 0.012322570197284222, 0.012080139480531216, 0.012246307916939259, 0.012517995201051235, 0.012518567964434624, 0.01254206895828247], [0.012809696607291698, 0.012897438369691372, 0.012750869616866112, 0.012773077934980392, 0.012587425298988819, 0.012612657621502876, 0.01280146837234497, 0.012651399709284306, 0.012649201788008213, 0.012848222628235817, 0.012788419611752033, 0.01272675208747387, 0.012645471841096878, 0.012511971406638622, 0.012542981654405594, 0.012620160356163979, 0.012322572059929371, 0.012462305836379528, 0.012648562900722027, 0.012663665227591991, 0.012544293887913227, 0.012401819229125977, 0.012322649359703064, 0.012080223299562931, 0.012246405705809593, 0.012518084608018398, 0.01251864992082119, 0.012542118318378925], [0.012809625826776028, 0.01289733499288559, 0.0127507783472538, 0.012772933579981327, 0.012587303295731544, 0.01261256355792284, 0.012801405973732471, 0.01265126932412386, 0.012649103067815304, 0.012848066166043282, 0.012788314372301102, 0.012726681306958199, 0.012645403854548931, 0.012511908076703548, 0.01254284381866455, 0.012620031833648682, 0.012322485446929932, 0.012462192215025425, 0.012648492120206356, 0.012663524597883224, 0.012544166296720505, 0.012401733547449112, 0.012322545982897282, 0.01208017859607935, 0.012246331200003624, 0.012517962604761124, 0.012518602423369884, 0.012542029842734337], [0.012809720821678638, 0.012897448614239693, 0.012750905938446522, 0.01277309749275446, 0.012587441131472588, 0.012612656690180302, 0.012801503762602806, 0.012651408091187477, 0.012649234384298325, 0.012848242186009884, 0.012788452208042145, 0.012726757675409317, 0.012645498849451542, 0.012512007728219032, 0.012543007731437683, 0.01262016873806715, 0.012322590686380863, 0.012462315149605274, 0.012648578733205795, 0.012663674540817738, 0.012544305063784122, 0.012401857413351536, 0.012322667986154556, 0.012080240994691849, 0.012246418744325638, 0.012518102303147316, 0.012518673203885555, 0.012542124837636948], [0.012809624895453453, 0.012897354550659657, 0.012750806286931038, 0.012772990390658379, 0.012587340548634529, 0.01261257752776146, 0.01280139945447445, 0.012651313096284866, 0.012649154290556908, 0.012848124839365482, 0.012788367457687855, 0.012726690620183945, 0.012645423412322998, 0.012511935085058212, 0.0125429043546319, 0.012620043009519577, 0.012322497554123402, 0.012462204322218895, 0.01264848094433546, 0.012663573026657104, 0.012544193305075169, 0.012401765212416649, 0.01232259813696146, 0.012080159038305283, 0.012246329337358475, 0.012517993338406086, 0.012518600560724735, 0.012542063370347023], [0.01280966866761446, 0.012897408567368984, 0.012750820256769657, 0.012773023918271065, 0.01258736103773117, 0.012612588703632355, 0.012801413424313068, 0.012651352211833, 0.012649157084524632, 0.012848169542849064, 0.012788394466042519, 0.012726688757538795, 0.01264545600861311, 0.012511963956058025, 0.012542914599180222, 0.012620112858712673, 0.012322485446929932, 0.012462232261896133, 0.01264855358749628, 0.012663623318076134, 0.012544259428977966, 0.01240181177854538, 0.012322627939283848, 0.01208017859607935, 0.012246361933648586, 0.012518025934696198, 0.012518624775111675, 0.012542089447379112], [0.012809720821678638, 0.012897471897304058, 0.012750870548188686, 0.012773077003657818, 0.01258742157369852, 0.012612603604793549, 0.012801462784409523, 0.012651361525058746, 0.012649166397750378, 0.012848158366978168, 0.01278840471059084, 0.012726716697216034, 0.012645465321838856, 0.012511970475316048, 0.012542955577373505, 0.012620117515325546, 0.012322550639510155, 0.012462298385798931, 0.012648559175431728, 0.01266366895288229, 0.012544279918074608, 0.012401828542351723, 0.012322654016315937, 0.012080248445272446, 0.012246430851519108, 0.012518089264631271, 0.012518670409917831, 0.01254214160144329], [0.012809691950678825, 0.012897423468530178, 0.01275086123496294, 0.012773065827786922, 0.012587398290634155, 0.012612632475793362, 0.012801475822925568, 0.012651372700929642, 0.012649193406105042, 0.012848223559558392, 0.01278840471059084, 0.012726746499538422, 0.012645444832742214, 0.01251200307160616, 0.01254298910498619, 0.012620166875422001, 0.012322546914219856, 0.012462291866540909, 0.012648575939238071, 0.01266367919743061, 0.012544253841042519, 0.012401794083416462, 0.012322664260864258, 0.012080234475433826, 0.012246391735970974, 0.01251806877553463, 0.012518640607595444, 0.012542123906314373], [0.01280962023884058, 0.012897375039756298, 0.012750792317092419, 0.012773037888109684, 0.01258736290037632, 0.012612592428922653, 0.012801450677216053, 0.012651359662413597, 0.01264917477965355, 0.01284814439713955, 0.012788396328687668, 0.012726682238280773, 0.012645429000258446, 0.01251195464283228, 0.01254291646182537, 0.012620048597455025, 0.012322511523962021, 0.012462222948670387, 0.01264849491417408, 0.012663591653108597, 0.012544245459139347, 0.012401789426803589, 0.012322608381509781, 0.012080177664756775, 0.012246371246874332, 0.012518012896180153, 0.012518647126853466, 0.012542078271508217], [0.012809738516807556, 0.012897436507046223, 0.012750916182994843, 0.012773090973496437, 0.012587429024279118, 0.012612654827535152, 0.012801475822925568, 0.012651382014155388, 0.012649252079427242, 0.012848232872784138, 0.012788468040525913, 0.012726781889796257, 0.012645521201193333, 0.012512050569057465, 0.012543005868792534, 0.012620150111615658, 0.01232257578521967, 0.012462294660508633, 0.012648601084947586, 0.012663673609495163, 0.012544317170977592, 0.012401889078319073, 0.012322704307734966, 0.012080264277756214, 0.012246442958712578, 0.01251809298992157, 0.012518703006207943, 0.01254213321954012], [0.012809701263904572, 0.012897385284304619, 0.012750848196446896, 0.012773066759109497, 0.012587378732860088, 0.012612604536116123, 0.01280144415795803, 0.012651399709284306, 0.0126491729170084, 0.01284818071871996, 0.012788400985300541, 0.012726761400699615, 0.012645483948290348, 0.012511982582509518, 0.012542923912405968, 0.012620143592357635, 0.012322547845542431, 0.012462257407605648, 0.012648547068238258, 0.012663646601140499, 0.01254425011575222, 0.01240179967135191, 0.012322633527219296, 0.012080180458724499, 0.012246372178196907, 0.012518023140728474, 0.01251864992082119, 0.012542136013507843], [0.012809627689421177, 0.01289734710007906, 0.012750793248414993, 0.012773003429174423, 0.01258736290037632, 0.012612594291567802, 0.012801422737538815, 0.012651341035962105, 0.01264917105436325, 0.012848160229623318, 0.012788387015461922, 0.012726682238280773, 0.012645450420677662, 0.012511935085058212, 0.012542909942567348, 0.012620109133422375, 0.012322517111897469, 0.012462235987186432, 0.012648524716496468, 0.01266357209533453, 0.012544205412268639, 0.012401790358126163, 0.01232262421399355, 0.012080165557563305, 0.01224635262042284, 0.012518016621470451, 0.012518614530563354, 0.012542083859443665], [0.01280966866761446, 0.012897400185465813, 0.012750823050737381, 0.0127730006352067, 0.012587359175086021, 0.012612637132406235, 0.012801437638700008, 0.012651367112994194, 0.012649194337427616, 0.012848149053752422, 0.012788345105946064, 0.012726709246635437, 0.012645450420677662, 0.01251197513192892, 0.012542925775051117, 0.012620097026228905, 0.012322545051574707, 0.01246226392686367, 0.012648558244109154, 0.012663638219237328, 0.012544278055429459, 0.012401808053255081, 0.012322654947638512, 0.012080226093530655, 0.012246434576809406, 0.01251805480569601, 0.012518668547272682, 0.0125421192497015], [0.012809637002646923, 0.01289734710007906, 0.012750793248414993, 0.012772966176271439, 0.012587321922183037, 0.012612549588084221, 0.012801414355635643, 0.012651316821575165, 0.012649118900299072, 0.012848143465816975, 0.012788343243300915, 0.012726673856377602, 0.012645418755710125, 0.01251191832125187, 0.012542909011244774, 0.0126200495287776, 0.012322455644607544, 0.01246219128370285, 0.012648487463593483, 0.012663548812270164, 0.01254415512084961, 0.012401755899190903, 0.012322544120252132, 0.012080131098628044, 0.012246294878423214, 0.012517974711954594, 0.012518566101789474, 0.012542050331830978], [0.012809665873646736, 0.012897416949272156, 0.012750848196446896, 0.012773026712238789, 0.012587377801537514, 0.012612623162567616, 0.012801469303667545, 0.01265137828886509, 0.012649187818169594, 0.012848177924752235, 0.012788398191332817, 0.012726742774248123, 0.012645474635064602, 0.012511980719864368, 0.012542948126792908, 0.012620103545486927, 0.012322526425123215, 0.012462249957025051, 0.012648550793528557, 0.012663623318076134, 0.012544271536171436, 0.012401806190609932, 0.012322647497057915, 0.01208021305501461, 0.01224641501903534, 0.012518038973212242, 0.012518634088337421, 0.012542089447379112], [0.01280971523374319, 0.012897426262497902, 0.01275087520480156, 0.012773036025464535, 0.012587395496666431, 0.01261263806372881, 0.012801473028957844, 0.01265138853341341, 0.01264920737594366, 0.01284821517765522, 0.012788395397365093, 0.012726765125989914, 0.012645482085645199, 0.01251199934631586, 0.0125429667532444, 0.01262014452368021, 0.012322545982897282, 0.012462290935218334, 0.012648575939238071, 0.012663649395108223, 0.012544275261461735, 0.012401841580867767, 0.012322666123509407, 0.012080234475433826, 0.012246410362422466, 0.012518060393631458, 0.012518690899014473, 0.01254214532673359], [0.012809704057872295, 0.01289743185043335, 0.012750877067446709, 0.012773090042173862, 0.012587422505021095, 0.012612633407115936, 0.01280148234218359, 0.012651398777961731, 0.012649217620491982, 0.012848197482526302, 0.012788412161171436, 0.012726705521345139, 0.012645459733903408, 0.012511969543993473, 0.012543008662760258, 0.012620160356163979, 0.012322555296123028, 0.012462284415960312, 0.012648560106754303, 0.01266369130462408, 0.012544311583042145, 0.012401828542351723, 0.01232266053557396, 0.012080231681466103, 0.012246432714164257, 0.012518076226115227, 0.012518678791821003, 0.012542148120701313], [0.012809612788259983, 0.012897316366434097, 0.012750797905027866, 0.01277301274240017, 0.012587331235408783, 0.012612568214535713, 0.0128013975918293, 0.012651287019252777, 0.012649131938815117, 0.012848097831010818, 0.012788327410817146, 0.012726678512990475, 0.012645438313484192, 0.012511931359767914, 0.012542888522148132, 0.01262007188051939, 0.01232249103486538, 0.012462209910154343, 0.012648483738303185, 0.012663592584431171, 0.012544182129204273, 0.012401742860674858, 0.01232256181538105, 0.012080189771950245, 0.012246322818100452, 0.012518010102212429, 0.012518627569079399, 0.012542099691927433], [0.012809660285711288, 0.0128974299877882, 0.012750857509672642, 0.012773027643561363, 0.012587431818246841, 0.01261268649250269, 0.012801503762602806, 0.012651351280510426, 0.012649193406105042, 0.012848182581365108, 0.012788433581590652, 0.012726805172860622, 0.012645515613257885, 0.012512014247477055, 0.012542933225631714, 0.012620116584002972, 0.012322512455284595, 0.012462248094379902, 0.012648544274270535, 0.012663614936172962, 0.012544321827590466, 0.012401822954416275, 0.012322656810283661, 0.012080175802111626, 0.012246417813003063, 0.01251805666834116, 0.012518641538918018, 0.012542086653411388], [0.01280971895903349, 0.012897448614239693, 0.01275088544934988, 0.012773074209690094, 0.012587439268827438, 0.012612642720341682, 0.012801505625247955, 0.012651408091187477, 0.012649194337427616, 0.012848207727074623, 0.01278842892497778, 0.012726745568215847, 0.012645475566387177, 0.01251196302473545, 0.012542994692921638, 0.012620147317647934, 0.01232253946363926, 0.012462283484637737, 0.012648560106754303, 0.01266365684568882, 0.012544271536171436, 0.012401842512190342, 0.012322650291025639, 0.012080220505595207, 0.01224640291184187, 0.012518076226115227, 0.012518645264208317, 0.012542134150862694], [0.012809701263904572, 0.012897445820271969, 0.012750860303640366, 0.01277304720133543, 0.01258739735931158, 0.0126126017421484, 0.012801464647054672, 0.012651391327381134, 0.012649192474782467, 0.012848172336816788, 0.012788385152816772, 0.01272668968886137, 0.012645451352000237, 0.01251197513192892, 0.01254295464605093, 0.012620119377970695, 0.012322521768510342, 0.012462248094379902, 0.012648546136915684, 0.012663649395108223, 0.0125442398712039, 0.01240179967135191, 0.012322617694735527, 0.012080210261046886, 0.012246371246874332, 0.012518070638179779, 0.012518663890659809, 0.012542144395411015], [0.012809700332581997, 0.01289744209498167, 0.01275091152638197, 0.012773030437529087, 0.012587405741214752, 0.012612646445631981, 0.01280150469392538, 0.012651435099542141, 0.012649214826524258, 0.012848181650042534, 0.012788425199687481, 0.01272675022482872, 0.012645472772419453, 0.012511963956058025, 0.01254294067621231, 0.012620173394680023, 0.012322562746703625, 0.01246228814125061, 0.012648582458496094, 0.012663638219237328, 0.012544257566332817, 0.012401805259287357, 0.01232264656573534, 0.012080233544111252, 0.01224640291184187, 0.012518083676695824, 0.012518628500401974, 0.012542135082185268], [0.012809704057872295, 0.01289744395762682, 0.01275086309760809, 0.012773089110851288, 0.012587427161633968, 0.012612652033567429, 0.012801476754248142, 0.01265142485499382, 0.012649213895201683, 0.012848209589719772, 0.012788430787622929, 0.012726769782602787, 0.012645522132515907, 0.012512004002928734, 0.012542977929115295, 0.012620129622519016, 0.012322585098445415, 0.012462313286960125, 0.012648576870560646, 0.012663658708333969, 0.012544278986752033, 0.012401840649545193, 0.012322669848799706, 0.012080228887498379, 0.012246418744325638, 0.012518081814050674, 0.012518662959337234, 0.01254212111234665], [0.012809716165065765, 0.0128974299877882, 0.012750889174640179, 0.01277303695678711, 0.012587420642375946, 0.01261263806372881, 0.012801463715732098, 0.012651403434574604, 0.012649228796362877, 0.012848231010138988, 0.012788432650268078, 0.012726716697216034, 0.012645510025322437, 0.012511953711509705, 0.012542963959276676, 0.012620166875422001, 0.012322568334639072, 0.012462296523153782, 0.012648564763367176, 0.012663627974689007, 0.012544280849397182, 0.01240186020731926, 0.012322704307734966, 0.012080232612788677, 0.012246421538293362, 0.01251807902008295, 0.012518635019659996, 0.012542110867798328], [0.01280960626900196, 0.012897362932562828, 0.012750795111060143, 0.012772968038916588, 0.01258732657879591, 0.012612567283213139, 0.012801406905055046, 0.012651271186769009, 0.012649129144847393, 0.012848121114075184, 0.012788358144462109, 0.012726654298603535, 0.012645424343645573, 0.012511931359767914, 0.012542893178761005, 0.01262008585035801, 0.012322483584284782, 0.012462200596928596, 0.012648472562432289, 0.012663559056818485, 0.012544164434075356, 0.012401729822158813, 0.012322570197284222, 0.012080139480531216, 0.012246307916939259, 0.012517995201051235, 0.012518567033112049, 0.01254206895828247], [0.012809696607291698, 0.012897438369691372, 0.012750869616866112, 0.012773077934980392, 0.012587425298988819, 0.012612659484148026, 0.01280146837234497, 0.012651399709284306, 0.012649201788008213, 0.012848222628235817, 0.012788419611752033, 0.01272675208747387, 0.012645471841096878, 0.012511971406638622, 0.012542981654405594, 0.012620160356163979, 0.012322572059929371, 0.012462305836379528, 0.012648562900722027, 0.012663665227591991, 0.012544293887913227, 0.012401819229125977, 0.012322649359703064, 0.012080223299562931, 0.012246405705809593, 0.012518084608018398, 0.01251864992082119, 0.012542118318378925], [0.012809625826776028, 0.01289733499288559, 0.0127507783472538, 0.012772933579981327, 0.012587303295731544, 0.01261256355792284, 0.012801405973732471, 0.01265126932412386, 0.012649103067815304, 0.012848066166043282, 0.012788314372301102, 0.012726681306958199, 0.012645403854548931, 0.012511908076703548, 0.01254284381866455, 0.012620031833648682, 0.012322485446929932, 0.012462192215025425, 0.012648492120206356, 0.012663524597883224, 0.012544166296720505, 0.012401733547449112, 0.012322545982897282, 0.01208017859607935, 0.012246331200003624, 0.012517962604761124, 0.012518602423369884, 0.012542029842734337], [0.012809720821678638, 0.012897448614239693, 0.012750905938446522, 0.01277309749275446, 0.012587441131472588, 0.012612656690180302, 0.012801503762602806, 0.012651408091187477, 0.012649234384298325, 0.012848242186009884, 0.012788452208042145, 0.012726757675409317, 0.012645498849451542, 0.012512007728219032, 0.012543007731437683, 0.01262016873806715, 0.012322590686380863, 0.012462315149605274, 0.012648578733205795, 0.012663674540817738, 0.012544305063784122, 0.012401857413351536, 0.012322667986154556, 0.012080240994691849, 0.012246418744325638, 0.012518102303147316, 0.012518673203885555, 0.012542124837636948], [0.012809624895453453, 0.012897354550659657, 0.012750806286931038, 0.012772990390658379, 0.012587340548634529, 0.01261257752776146, 0.01280139945447445, 0.012651313096284866, 0.012649154290556908, 0.012848124839365482, 0.012788367457687855, 0.012726690620183945, 0.012645423412322998, 0.012511935085058212, 0.0125429043546319, 0.012620043009519577, 0.012322497554123402, 0.012462204322218895, 0.01264848094433546, 0.012663573026657104, 0.012544193305075169, 0.012401765212416649, 0.01232259813696146, 0.012080159038305283, 0.012246329337358475, 0.012517993338406086, 0.012518600560724735, 0.012542063370347023], [0.01280966866761446, 0.012897408567368984, 0.012750820256769657, 0.012773023918271065, 0.01258736103773117, 0.012612588703632355, 0.012801413424313068, 0.012651352211833, 0.012649157084524632, 0.012848169542849064, 0.012788394466042519, 0.012726688757538795, 0.01264545600861311, 0.012511963956058025, 0.012542914599180222, 0.012620112858712673, 0.012322485446929932, 0.012462232261896133, 0.01264855358749628, 0.012663623318076134, 0.012544259428977966, 0.01240181177854538, 0.012322627939283848, 0.01208017859607935, 0.012246361933648586, 0.012518025934696198, 0.012518624775111675, 0.012542089447379112], [0.012809720821678638, 0.012897471897304058, 0.012750870548188686, 0.012773077003657818, 0.01258742157369852, 0.012612603604793549, 0.012801462784409523, 0.012651361525058746, 0.012649166397750378, 0.012848158366978168, 0.01278840471059084, 0.012726716697216034, 0.012645465321838856, 0.012511970475316048, 0.012542955577373505, 0.012620117515325546, 0.012322550639510155, 0.012462298385798931, 0.012648559175431728, 0.01266366895288229, 0.012544279918074608, 0.012401828542351723, 0.012322654016315937, 0.012080248445272446, 0.012246430851519108, 0.012518089264631271, 0.012518670409917831, 0.01254214160144329], [0.012809691950678825, 0.012897423468530178, 0.01275086123496294, 0.012773065827786922, 0.012587398290634155, 0.012612632475793362, 0.012801475822925568, 0.012651372700929642, 0.012649193406105042, 0.012848223559558392, 0.01278840471059084, 0.012726746499538422, 0.012645444832742214, 0.01251200307160616, 0.01254298910498619, 0.012620166875422001, 0.012322546914219856, 0.012462291866540909, 0.012648575939238071, 0.01266367919743061, 0.012544253841042519, 0.012401794083416462, 0.012322664260864258, 0.012080234475433826, 0.012246391735970974, 0.01251806877553463, 0.012518640607595444, 0.012542123906314373], [0.01280962023884058, 0.012897375039756298, 0.012750792317092419, 0.012773037888109684, 0.01258736290037632, 0.012612592428922653, 0.012801450677216053, 0.012651359662413597, 0.01264917477965355, 0.01284814439713955, 0.012788396328687668, 0.012726682238280773, 0.012645429000258446, 0.01251195464283228, 0.01254291646182537, 0.012620048597455025, 0.012322511523962021, 0.012462222948670387, 0.01264849491417408, 0.012663591653108597, 0.012544245459139347, 0.012401789426803589, 0.012322608381509781, 0.012080177664756775, 0.012246371246874332, 0.012518012896180153, 0.012518647126853466, 0.012542078271508217], [0.012809738516807556, 0.012897436507046223, 0.012750916182994843, 0.012773090973496437, 0.012587429024279118, 0.012612654827535152, 0.012801475822925568, 0.012651382014155388, 0.012649252079427242, 0.012848232872784138, 0.012788468040525913, 0.012726781889796257, 0.012645521201193333, 0.012512050569057465, 0.012543005868792534, 0.012620150111615658, 0.01232257578521967, 0.012462294660508633, 0.012648601084947586, 0.012663673609495163, 0.012544317170977592, 0.012401889078319073, 0.012322704307734966, 0.012080264277756214, 0.012246442958712578, 0.01251809298992157, 0.012518703006207943, 0.01254213321954012], [0.012809701263904572, 0.012897385284304619, 0.012750848196446896, 0.012773066759109497, 0.012587378732860088, 0.012612604536116123, 0.01280144415795803, 0.012651399709284306, 0.0126491729170084, 0.01284818071871996, 0.012788400985300541, 0.012726761400699615, 0.012645483948290348, 0.012511982582509518, 0.012542923912405968, 0.012620143592357635, 0.012322547845542431, 0.012462257407605648, 0.012648547068238258, 0.012663646601140499, 0.01254425011575222, 0.01240179967135191, 0.012322633527219296, 0.012080180458724499, 0.012246372178196907, 0.012518023140728474, 0.01251864992082119, 0.012542136013507843], [0.012809627689421177, 0.01289734710007906, 0.012750793248414993, 0.012773003429174423, 0.01258736290037632, 0.012612594291567802, 0.012801422737538815, 0.012651341035962105, 0.01264917105436325, 0.012848160229623318, 0.012788387015461922, 0.012726682238280773, 0.012645450420677662, 0.012511935085058212, 0.012542909942567348, 0.012620109133422375, 0.012322517111897469, 0.012462235987186432, 0.012648524716496468, 0.01266357209533453, 0.012544205412268639, 0.012401790358126163, 0.01232262421399355, 0.012080165557563305, 0.01224635262042284, 0.012518016621470451, 0.012518614530563354, 0.012542083859443665], [0.01280966866761446, 0.012897400185465813, 0.012750823050737381, 0.0127730006352067, 0.012587359175086021, 0.012612637132406235, 0.012801437638700008, 0.012651367112994194, 0.012649194337427616, 0.012848149053752422, 0.012788345105946064, 0.012726709246635437, 0.012645450420677662, 0.01251197513192892, 0.012542925775051117, 0.012620097026228905, 0.012322545051574707, 0.01246226392686367, 0.012648558244109154, 0.012663638219237328, 0.012544278055429459, 0.012401808053255081, 0.012322654947638512, 0.012080226093530655, 0.012246434576809406, 0.01251805480569601, 0.012518668547272682, 0.0125421192497015], [0.012809637002646923, 0.01289734710007906, 0.012750793248414993, 0.012772966176271439, 0.012587321922183037, 0.012612549588084221, 0.012801414355635643, 0.012651316821575165, 0.012649118900299072, 0.012848143465816975, 0.012788343243300915, 0.012726673856377602, 0.012645418755710125, 0.01251191832125187, 0.012542909011244774, 0.0126200495287776, 0.012322455644607544, 0.01246219128370285, 0.012648487463593483, 0.012663548812270164, 0.01254415512084961, 0.012401755899190903, 0.012322544120252132, 0.012080131098628044, 0.012246294878423214, 0.012517974711954594, 0.012518566101789474, 0.012542050331830978]]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d1310495-6e38-4a9d-9693-acf502a86cc8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = go.Heatmap(z = attention, colorscale='Reds')\n",
    "data=[train]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n",
       "array([[[0.20362999],\n",
       "        [0.2018812 ],\n",
       "        [0.1988714 ],\n",
       "        [0.19788313],\n",
       "        [0.19773428]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 20\n",
    "units = 40\n",
    "x = np.expand_dims(np.random.randint(0, vocab_size, (5)), axis=0)\n",
    "\n",
    "# passing encoder\n",
    "embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "x = embedding(x)\n",
    "gru_cell = gru(units)\n",
    "output, state = gru_cell(x)\n",
    "\n",
    "# passing decoder\n",
    "W1 = tf.keras.layers.Dense(units)\n",
    "W2 = tf.keras.layers.Dense(units)\n",
    "V = tf.keras.layers.Dense(1)\n",
    "\n",
    "hidden_with_time_axis = tf.expand_dims(state, 1)\n",
    "score = V(tf.nn.tanh(W1(output) + W2(hidden_with_time_axis)))\n",
    "\n",
    "# attention_weights shape == (batch_size, max_length, 1)\n",
    "attention_weights = tf.nn.softmax(score, axis=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 5, 40]), TensorShape([1, 40]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 40])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(W1(output) + W2(state)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = attention_weights * output\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
       "array([[ 0.00450457, -0.000479  ,  0.01776177, -0.00874093, -0.00549831,\n",
       "        -0.00403002, -0.00280772,  0.01280778, -0.00150356,  0.01241918,\n",
       "        -0.01647402, -0.00602317,  0.00513975,  0.00845177, -0.01333129,\n",
       "         0.00751389,  0.00228994,  0.01548713, -0.00374114,  0.00115751,\n",
       "        -0.00692876,  0.00238407, -0.00133134,  0.01229093,  0.01858362,\n",
       "        -0.00297811, -0.0060442 ,  0.00627679,  0.00151788,  0.01066009,\n",
       "        -0.02028783,  0.01406133, -0.02786346,  0.01104608,  0.01019504,\n",
       "         0.00855909, -0.0151533 , -0.00260982,  0.02325839,  0.00457436]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n",
       "array([[[0.20131408],\n",
       "        [0.20278044],\n",
       "        [0.19993325],\n",
       "        [0.1975839 ],\n",
       "        [0.19838838]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/data/korean-parallel-corpora/korean-english-news-v1/training_checkpoints/ckpt-10.\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                   save_weights_only=True,\n",
    "                                   verbose=1)# Train the model with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 낙서장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from modeling.albert_modeling import AlbertMultiHeadAttention\n",
    "from utils import get_num_params\n",
    "from config.albert_config import AlbertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 40])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = AlbertConfig()\n",
    "np.random.seed(100)\n",
    "n_batch = 3\n",
    "inp = np.random.random((n_batch, 10, conf.n_hidden))\n",
    "inp = torch.FloatTensor(inp)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([3, 10, 40])\n",
      "tensor(-1.8835e-05, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "self_attention = AlbertMultiHeadAttention(conf)\n",
    "out = self_attention(inp)\n",
    "print('output shape: {}'.format(out.shape))\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qo = self_attention.query(inp)\n",
    "ko = self_attention.key(inp)\n",
    "vo = self_attention.value(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 8, 5]), torch.Size([3, 8, 10, 5]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention.split_tensor(qo).shape, self_attention.split_tensor(qo).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qo = torch.Size([3, 10, 8, 5]<br>\n",
    "→ 문장이 3개가 있고, <br>\n",
    "→ 각 문장은 10개의 단어로 이루어져 있다.<br>\n",
    "→ 그런데 각각의 단어는 그 단어를 표현하는 8개의 Attention으로 이루어져 있고,<br>\n",
    "→ 그 Attention의 사이즈는 5이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 10]), torch.Size([3, 10, 40]), torch.Size([3, 40, 10]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qo @ ko.transpose(-2,-1)).shape, qo.shape, ko.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 10]), torch.Size([3, 10, 10]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = (qo @ ko.transpose(-2,-1)) / np.sqrt(self_attention.attention_size)\n",
    "prob = self_attention.softmax(score)\n",
    "score.shape, prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 40])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prob @ vo).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ba5093ff901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgluonnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencepieceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkogpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/gluonnlp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\"\"\"NLP toolkit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/gluonnlp/loss/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\"\"\"NLP loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mactivation_regularizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/git/LaH/env/lib/python3.6/site-packages/gluonnlp/loss/activation_regularizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ActivationRegularizationLoss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TemporalActivationRegularizationLoss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kogpt2.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_path = get_tokenizer()\n",
    "tok = SentencepieceTokenizer(tok_path)\n",
    "sent = '2019년 한해를 보내며,'\n",
    "toked = tok(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lah",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
