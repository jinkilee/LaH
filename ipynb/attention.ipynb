{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:\n",
    "- https://github.com/jungyeul/korean-parallel-corpora\n",
    "- https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concatenate kor.txt and eng.txt with tab delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import sklearn\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotly.offline import iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/korean-parallel-corpora/korean-english-news-v1'\n",
    "targets = ['train', 'test', 'dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets:\n",
    "    output_filename = '{}/{}.txt'.format(data_dir, target)\n",
    "    if os.path.exists(output_filename):\n",
    "        continue\n",
    "        \n",
    "    ko_file, en_file = glob.glob('{}/*{}.??'.format(data_dir, target))\n",
    "    \n",
    "    # read korean/english files\n",
    "    ko_lines = open(ko_file).read().strip().split('\\n')\n",
    "    en_lines = open(en_file).read().strip().split('\\n')\n",
    "    \n",
    "    # write to output file\n",
    "    with open(output_filename, 'w') as out:\n",
    "        for en, kr in zip(en_lines, ko_lines):\n",
    "            oneline = '\\t'.join([en, kr])\n",
    "            out.write(oneline + '\\n')\n",
    "            \n",
    "    print('{} was written'.format(output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path\n",
    "file_path = os.path.join(data_dir, 'train.txt')\n",
    "\n",
    "# read the file\n",
    "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
    "lines = lines[:20000]\n",
    "\n",
    "# perform basic cleaning\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "remove_digits = str.maketrans('', '', string.digits) # Set of all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kor_sentence(sent):\n",
    "    '''Function to preprocess Marathi sentence'''\n",
    "    sent = re.sub(\"'\", '', sent)\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent\n",
    "\n",
    "def preprocess_eng_sentence(sent):\n",
    "    '''Function to preprocess English sentence'''\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(\"'\", '', sent)\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
    "    sent = sent.translate(remove_digits)\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\" +\", \" \", sent)\n",
    "    sent = '<start> ' + sent + ' <end>'\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairs of cleaned English and Marathi sentences\n",
    "sent_pairs = []\n",
    "for line in lines:\n",
    "    sent_pair = []\n",
    "    ko, en = line.split('\\t')\n",
    "    \n",
    "    # append korean\n",
    "    ko = preprocess_kor_sentence(ko)\n",
    "    sent_pair.append(ko)\n",
    "    \n",
    "    # append english\n",
    "    en = preprocess_kor_sentence(en)\n",
    "    sent_pair.append(en)\n",
    "    \n",
    "    # append sentence pair\n",
    "    sent_pairs.append(sent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate maximum length of the sequence\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(pairs, num_examples):\n",
    "    # pairs => already created cleaned input, output pairs\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
    "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # English sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # Marathi sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tensors\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters of the model\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 8\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch generator to be used by modle to load data in batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        # this is the step 1 described in the blog to compute scores s1, s2, ...\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # this is the step 2 described in the blog to compute attention weights e1, e2, ...\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # this is the step 3 described in the blog to compute the context_vector = e1*h1 + e2*h2 + ...\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # this is the step 4 described in the blog to concatenate the context vector with the output of the previous time step\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        # this is the step 5 in the blog, to compute the next output word in the sequence\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        # return current output, current state and the attention weights\n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-84cdabcc053e>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create objects of Class Encoder and Class Decoder\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '{}/training_checkpoints'.format(data_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.6870\n",
      "Epoch 1 Batch 100 Loss 2.4057\n",
      "Epoch 1 Batch 200 Loss 2.1183\n",
      "Epoch 1 Batch 300 Loss 2.6536\n",
      "Epoch 1 Batch 400 Loss 1.8935\n",
      "Epoch 1 Batch 500 Loss 1.8911\n",
      "Epoch 1 Batch 600 Loss 2.4808\n",
      "Epoch 1 Batch 700 Loss 2.6916\n",
      "Epoch 1 Batch 800 Loss 2.4797\n",
      "Epoch 1 Batch 900 Loss 2.4831\n",
      "Epoch 1 Batch 1000 Loss 1.6847\n",
      "Epoch 1 Batch 1100 Loss 2.0072\n",
      "Epoch 1 Batch 1200 Loss 2.2642\n",
      "Epoch 1 Batch 1300 Loss 2.4417\n",
      "Epoch 1 Batch 1400 Loss 2.9924\n",
      "Epoch 1 Batch 1500 Loss 2.4839\n",
      "Epoch 1 Batch 1600 Loss 2.3007\n",
      "Epoch 1 Batch 1700 Loss 2.1889\n",
      "Epoch 1 Batch 1800 Loss 2.0863\n",
      "Epoch 1 Batch 1900 Loss 1.3945\n",
      "Epoch 1 Batch 2000 Loss 1.9467\n",
      "Epoch 1 Batch 2100 Loss 2.0779\n",
      "Epoch 1 Batch 2200 Loss 2.6295\n",
      "Epoch 1 Loss 2.2914\n",
      "Time taken for 1 epoch 2752.7094326019287 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8727\n",
      "Epoch 2 Batch 100 Loss 2.1798\n",
      "Epoch 2 Batch 200 Loss 1.9619\n",
      "Epoch 2 Batch 300 Loss 2.3598\n",
      "Epoch 2 Batch 400 Loss 2.2047\n",
      "Epoch 2 Batch 500 Loss 2.1206\n",
      "Epoch 2 Batch 600 Loss 1.9257\n",
      "Epoch 2 Batch 700 Loss 1.5613\n",
      "Epoch 2 Batch 800 Loss 2.4042\n",
      "Epoch 2 Batch 900 Loss 3.0817\n",
      "Epoch 2 Batch 1000 Loss 2.5659\n",
      "Epoch 2 Batch 1100 Loss 2.3049\n",
      "Epoch 2 Batch 1200 Loss 2.0366\n",
      "Epoch 2 Batch 1300 Loss 1.6451\n",
      "Epoch 2 Batch 1400 Loss 1.8970\n",
      "Epoch 2 Batch 1500 Loss 2.2283\n",
      "Epoch 2 Batch 1600 Loss 2.2511\n",
      "Epoch 2 Batch 1700 Loss 2.0989\n",
      "Epoch 2 Batch 1800 Loss 2.1267\n",
      "Epoch 2 Batch 1900 Loss 2.3785\n",
      "Epoch 2 Batch 2000 Loss 1.9535\n",
      "Epoch 2 Batch 2100 Loss 2.2048\n",
      "Epoch 2 Batch 2200 Loss 2.6389\n",
      "Epoch 2 Loss 2.1255\n",
      "Time taken for 1 epoch 2785.106153488159 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.9121\n",
      "Epoch 3 Batch 100 Loss 2.3136\n",
      "Epoch 3 Batch 200 Loss 2.3344\n",
      "Epoch 3 Batch 300 Loss 1.8046\n",
      "Epoch 3 Batch 400 Loss 2.0039\n",
      "Epoch 3 Batch 500 Loss 2.1177\n",
      "Epoch 3 Batch 600 Loss 1.9066\n",
      "Epoch 3 Batch 700 Loss 1.8721\n",
      "Epoch 3 Batch 800 Loss 2.2946\n",
      "Epoch 3 Batch 900 Loss 1.4828\n",
      "Epoch 3 Batch 1000 Loss 1.5941\n",
      "Epoch 3 Batch 1100 Loss 1.7196\n",
      "Epoch 3 Batch 1200 Loss 2.3066\n",
      "Epoch 3 Batch 1300 Loss 2.1974\n",
      "Epoch 3 Batch 1400 Loss 2.2151\n",
      "Epoch 3 Batch 1500 Loss 1.6719\n",
      "Epoch 3 Batch 1600 Loss 1.9753\n",
      "Epoch 3 Batch 1700 Loss 2.2134\n",
      "Epoch 3 Batch 1800 Loss 1.5569\n",
      "Epoch 3 Batch 1900 Loss 2.3480\n",
      "Epoch 3 Batch 2000 Loss 2.0083\n",
      "Epoch 3 Batch 2100 Loss 2.3794\n",
      "Epoch 3 Batch 2200 Loss 1.8043\n",
      "Epoch 3 Loss 2.0116\n",
      "Time taken for 1 epoch 2758.464434862137 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.0831\n",
      "Epoch 4 Batch 100 Loss 2.3580\n",
      "Epoch 4 Batch 200 Loss 1.9888\n",
      "Epoch 4 Batch 300 Loss 2.2810\n",
      "Epoch 4 Batch 400 Loss 2.0824\n",
      "Epoch 4 Batch 500 Loss 1.7515\n",
      "Epoch 4 Batch 600 Loss 1.7130\n",
      "Epoch 4 Batch 700 Loss 2.0222\n",
      "Epoch 4 Batch 800 Loss 1.5160\n",
      "Epoch 4 Batch 900 Loss 2.0293\n",
      "Epoch 4 Batch 1000 Loss 1.4578\n",
      "Epoch 4 Batch 1100 Loss 2.1201\n",
      "Epoch 4 Batch 1200 Loss 1.6412\n",
      "Epoch 4 Batch 1300 Loss 1.9089\n",
      "Epoch 4 Batch 1400 Loss 1.5557\n",
      "Epoch 4 Batch 1500 Loss 2.2110\n",
      "Epoch 4 Batch 1600 Loss 2.2002\n",
      "Epoch 4 Batch 1700 Loss 2.4488\n",
      "Epoch 4 Batch 1800 Loss 2.4122\n",
      "Epoch 4 Batch 1900 Loss 1.5108\n",
      "Epoch 4 Batch 2000 Loss 1.5676\n",
      "Epoch 4 Batch 2100 Loss 2.1121\n",
      "Epoch 4 Batch 2200 Loss 1.9446\n",
      "Epoch 4 Loss 1.8632\n",
      "Time taken for 1 epoch 2762.6702637672424 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.6130\n",
      "Epoch 5 Batch 100 Loss 1.2981\n",
      "Epoch 5 Batch 200 Loss 1.6166\n",
      "Epoch 5 Batch 300 Loss 1.5100\n",
      "Epoch 5 Batch 400 Loss 2.1840\n",
      "Epoch 5 Batch 500 Loss 1.7508\n",
      "Epoch 5 Batch 600 Loss 1.2878\n",
      "Epoch 5 Batch 700 Loss 1.3923\n",
      "Epoch 5 Batch 800 Loss 2.0242\n",
      "Epoch 5 Batch 900 Loss 1.5632\n",
      "Epoch 5 Batch 1000 Loss 2.3144\n",
      "Epoch 5 Batch 1100 Loss 1.8012\n",
      "Epoch 5 Batch 1200 Loss 1.8211\n",
      "Epoch 5 Batch 1300 Loss 1.4582\n",
      "Epoch 5 Batch 1400 Loss 1.8382\n",
      "Epoch 5 Batch 1500 Loss 1.8078\n",
      "Epoch 5 Batch 1600 Loss 1.9399\n",
      "Epoch 5 Batch 1700 Loss 1.6258\n",
      "Epoch 5 Batch 1800 Loss 1.4891\n",
      "Epoch 5 Batch 1900 Loss 1.4558\n",
      "Epoch 5 Batch 2000 Loss 1.7797\n",
      "Epoch 5 Batch 2100 Loss 1.5151\n",
      "Epoch 5 Batch 2200 Loss 1.6440\n",
      "Epoch 5 Loss 1.6522\n",
      "Time taken for 1 epoch 2764.892515182495 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.4738\n",
      "Epoch 6 Batch 100 Loss 1.2181\n",
      "Epoch 6 Batch 200 Loss 1.4532\n",
      "Epoch 6 Batch 300 Loss 1.3588\n",
      "Epoch 6 Batch 400 Loss 2.0264\n",
      "Epoch 6 Batch 500 Loss 1.7219\n",
      "Epoch 6 Batch 600 Loss 1.8444\n",
      "Epoch 6 Batch 700 Loss 1.2808\n",
      "Epoch 6 Batch 800 Loss 1.3648\n",
      "Epoch 6 Batch 900 Loss 1.4135\n",
      "Epoch 6 Batch 1000 Loss 1.1797\n",
      "Epoch 6 Batch 1100 Loss 1.2175\n",
      "Epoch 6 Batch 1200 Loss 1.3268\n",
      "Epoch 6 Batch 1300 Loss 1.3288\n",
      "Epoch 6 Batch 1400 Loss 1.6062\n",
      "Epoch 6 Batch 1500 Loss 1.2919\n",
      "Epoch 6 Batch 1600 Loss 1.2595\n",
      "Epoch 6 Batch 1700 Loss 1.5385\n",
      "Epoch 6 Batch 1800 Loss 1.4089\n",
      "Epoch 6 Batch 1900 Loss 2.3002\n",
      "Epoch 6 Batch 2000 Loss 1.6563\n",
      "Epoch 6 Batch 2100 Loss 1.3033\n",
      "Epoch 6 Batch 2200 Loss 1.4931\n",
      "Epoch 6 Loss 1.3895\n",
      "Time taken for 1 epoch 2764.469557285309 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.5071\n",
      "Epoch 7 Batch 100 Loss 1.0922\n",
      "Epoch 7 Batch 200 Loss 1.2484\n",
      "Epoch 7 Batch 300 Loss 1.4198\n",
      "Epoch 7 Batch 400 Loss 1.4371\n",
      "Epoch 7 Batch 500 Loss 0.5900\n",
      "Epoch 7 Batch 600 Loss 1.0092\n",
      "Epoch 7 Batch 700 Loss 0.8760\n",
      "Epoch 7 Batch 800 Loss 1.0388\n",
      "Epoch 7 Batch 900 Loss 1.0542\n",
      "Epoch 7 Batch 1000 Loss 1.2327\n",
      "Epoch 7 Batch 1100 Loss 1.2716\n",
      "Epoch 7 Batch 1200 Loss 1.0374\n",
      "Epoch 7 Batch 1300 Loss 1.1808\n",
      "Epoch 7 Batch 1400 Loss 1.1758\n",
      "Epoch 7 Batch 1500 Loss 0.9338\n",
      "Epoch 7 Batch 1600 Loss 1.0426\n",
      "Epoch 7 Batch 1700 Loss 1.2317\n",
      "Epoch 7 Batch 1800 Loss 0.8991\n",
      "Epoch 7 Batch 1900 Loss 1.1229\n",
      "Epoch 7 Batch 2000 Loss 1.2701\n",
      "Epoch 7 Batch 2100 Loss 1.0886\n",
      "Epoch 7 Batch 2200 Loss 1.0969\n",
      "Epoch 7 Loss 1.0954\n",
      "Time taken for 1 epoch 2754.667917728424 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0649\n",
      "Epoch 8 Batch 100 Loss 0.8858\n",
      "Epoch 8 Batch 200 Loss 0.7203\n",
      "Epoch 8 Batch 300 Loss 0.7923\n",
      "Epoch 8 Batch 400 Loss 0.8736\n",
      "Epoch 8 Batch 500 Loss 0.7328\n",
      "Epoch 8 Batch 600 Loss 0.6757\n",
      "Epoch 8 Batch 700 Loss 0.9886\n",
      "Epoch 8 Batch 800 Loss 1.0751\n",
      "Epoch 8 Batch 900 Loss 1.0156\n",
      "Epoch 8 Batch 1000 Loss 0.7509\n",
      "Epoch 8 Batch 1100 Loss 0.7280\n",
      "Epoch 8 Batch 1200 Loss 0.6828\n",
      "Epoch 8 Batch 1300 Loss 0.8201\n",
      "Epoch 8 Batch 1400 Loss 0.9125\n",
      "Epoch 8 Batch 1500 Loss 0.7933\n",
      "Epoch 8 Batch 1600 Loss 0.8899\n",
      "Epoch 8 Batch 1700 Loss 0.7936\n",
      "Epoch 8 Batch 1800 Loss 0.7940\n",
      "Epoch 8 Batch 1900 Loss 0.9603\n",
      "Epoch 8 Batch 2000 Loss 0.9449\n",
      "Epoch 8 Batch 2100 Loss 0.8309\n",
      "Epoch 8 Batch 2200 Loss 1.0404\n",
      "Epoch 8 Loss 0.8062\n",
      "Time taken for 1 epoch 2766.5168137550354 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5425\n",
      "Epoch 9 Batch 100 Loss 0.5449\n",
      "Epoch 9 Batch 200 Loss 0.7668\n",
      "Epoch 9 Batch 300 Loss 0.4949\n",
      "Epoch 9 Batch 400 Loss 0.6528\n",
      "Epoch 9 Batch 500 Loss 0.4973\n",
      "Epoch 9 Batch 600 Loss 0.4263\n",
      "Epoch 9 Batch 700 Loss 0.3350\n",
      "Epoch 9 Batch 800 Loss 0.5444\n",
      "Epoch 9 Batch 900 Loss 0.5506\n",
      "Epoch 9 Batch 1000 Loss 0.7238\n",
      "Epoch 9 Batch 1100 Loss 0.5647\n",
      "Epoch 9 Batch 1200 Loss 0.4267\n",
      "Epoch 9 Batch 1300 Loss 0.5195\n",
      "Epoch 9 Batch 1400 Loss 0.5058\n",
      "Epoch 9 Batch 1500 Loss 0.6201\n",
      "Epoch 9 Batch 1600 Loss 0.6131\n",
      "Epoch 9 Batch 1700 Loss 0.5914\n",
      "Epoch 9 Batch 1800 Loss 0.7539\n",
      "Epoch 9 Batch 1900 Loss 0.6950\n",
      "Epoch 9 Batch 2000 Loss 0.3486\n",
      "Epoch 9 Batch 2100 Loss 0.4633\n",
      "Epoch 9 Batch 2200 Loss 0.6910\n",
      "Epoch 9 Loss 0.5806\n",
      "Time taken for 1 epoch 2762.9758892059326 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2333\n",
      "Epoch 10 Batch 100 Loss 0.4145\n",
      "Epoch 10 Batch 200 Loss 0.2723\n",
      "Epoch 10 Batch 300 Loss 0.4681\n",
      "Epoch 10 Batch 400 Loss 0.4541\n",
      "Epoch 10 Batch 500 Loss 0.3541\n",
      "Epoch 10 Batch 600 Loss 0.4307\n",
      "Epoch 10 Batch 700 Loss 0.3495\n",
      "Epoch 10 Batch 800 Loss 0.3470\n",
      "Epoch 10 Batch 900 Loss 0.4705\n",
      "Epoch 10 Batch 1000 Loss 0.4734\n",
      "Epoch 10 Batch 1100 Loss 0.4480\n",
      "Epoch 10 Batch 1200 Loss 0.5491\n",
      "Epoch 10 Batch 1300 Loss 0.4019\n",
      "Epoch 10 Batch 1400 Loss 0.4924\n",
      "Epoch 10 Batch 1500 Loss 0.4184\n",
      "Epoch 10 Batch 1600 Loss 0.4402\n",
      "Epoch 10 Batch 1700 Loss 0.3715\n",
      "Epoch 10 Batch 1800 Loss 0.5365\n",
      "Epoch 10 Batch 1900 Loss 0.3968\n",
      "Epoch 10 Batch 2000 Loss 0.3894\n",
      "Epoch 10 Batch 2100 Loss 0.4752\n",
      "Epoch 10 Batch 2200 Loss 0.4539\n",
      "Epoch 10 Loss 0.4311\n",
      "Time taken for 1 epoch 2759.797627925873 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every epoch\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-11948d919a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/korean-parallel-corpora/korean-english-news-v1/training_checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    826\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    827\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    863\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "tf.disable_eager_execution()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('/data/korean-parallel-corpora/korean-english-news-v1/training_checkpoints'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
    "    sentence = sentence[:-1]\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    # start decoding\n",
    "    for t in range(max_length_targ): # limit the length of the decoded sequence\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        # stop decoding if '<end>' is predicted\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_val_sentence():\n",
    "    actual_sent = ''\n",
    "    k = np.random.randint(len(input_tensor_val))\n",
    "    random_input = input_tensor_val[k]\n",
    "    random_output = target_tensor_val[k]\n",
    "    random_input = np.expand_dims(random_input,0)\n",
    "    \n",
    "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "    print('Input: {}'.format(sentence[8:-6]))\n",
    "    print('Predicted translation: {}'.format(result[:-6]))\n",
    "    for i in random_output:\n",
    "        if i == 0:\n",
    "            break\n",
    "        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "    actual_sent = actual_sent[8:-7]\n",
    "    print('Actual translation: {}'.format(actual_sent))\n",
    "    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
    "    sentence, result = sentence.split(' '), result.split(' ')\n",
    "    sentence = sentence[1:-1]\n",
    "    result = result[:-2]\n",
    "    # use plotly to plot the heatmap\n",
    "    trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='Reds')\n",
    "    data=[trace]\n",
    "    iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [79,40], In[1]: [256,256] [Op:MatMul] name: decoder/dense_1/Tensordot/MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-04e5acb29461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Finally call the function multiple times to visualize random results from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_random_val_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-87d29214d308>\u001b[0m in \u001b[0;36mpredict_random_val_sentence\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrandom_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-31e1b16b2729>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# start decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# limit the length of the decoded sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# storing the attention weights to plot later on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-84cdabcc053e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# this is the step 1 described in the blog to compute scores s1, s2, ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_with_time_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# attention_weights shape == (batch_size, max_length, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0;31m# Broadcasting is required for the inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m       \u001b[0;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes, name)\u001b[0m\n\u001b[1;32m   4104\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[1;32m   4105\u001b[0m         b, b_axes, True)\n\u001b[0;32m-> 4106\u001b[0;31m     \u001b[0mab_matmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4108\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab_matmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_free_dims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2798\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5614\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5615\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5616\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5617\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5618\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anything/git/LaH/env_bert-like/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [79,40], In[1]: [256,256] [Op:MatMul] name: decoder/dense_1/Tensordot/MatMul/"
     ]
    }
   ],
   "source": [
    "# Finally call the function multiple times to visualize random results from the test set\n",
    "predict_random_val_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n",
       "array([[[0.20362999],\n",
       "        [0.2018812 ],\n",
       "        [0.1988714 ],\n",
       "        [0.19788313],\n",
       "        [0.19773428]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 20\n",
    "units = 40\n",
    "x = np.expand_dims(np.random.randint(0, vocab_size, (5)), axis=0)\n",
    "\n",
    "# passing encoder\n",
    "embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "x = embedding(x)\n",
    "gru_cell = gru(units)\n",
    "output, state = gru_cell(x)\n",
    "\n",
    "# passing decoder\n",
    "W1 = tf.keras.layers.Dense(units)\n",
    "W2 = tf.keras.layers.Dense(units)\n",
    "V = tf.keras.layers.Dense(1)\n",
    "\n",
    "hidden_with_time_axis = tf.expand_dims(state, 1)\n",
    "score = V(tf.nn.tanh(W1(output) + W2(hidden_with_time_axis)))\n",
    "\n",
    "# attention_weights shape == (batch_size, max_length, 1)\n",
    "attention_weights = tf.nn.softmax(score, axis=1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 5, 40]), TensorShape([1, 40]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 40])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(W1(output) + W2(state)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = attention_weights * output\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 40), dtype=float32, numpy=\n",
       "array([[ 0.00450457, -0.000479  ,  0.01776177, -0.00874093, -0.00549831,\n",
       "        -0.00403002, -0.00280772,  0.01280778, -0.00150356,  0.01241918,\n",
       "        -0.01647402, -0.00602317,  0.00513975,  0.00845177, -0.01333129,\n",
       "         0.00751389,  0.00228994,  0.01548713, -0.00374114,  0.00115751,\n",
       "        -0.00692876,  0.00238407, -0.00133134,  0.01229093,  0.01858362,\n",
       "        -0.00297811, -0.0060442 ,  0.00627679,  0.00151788,  0.01066009,\n",
       "        -0.02028783,  0.01406133, -0.02786346,  0.01104608,  0.01019504,\n",
       "         0.00855909, -0.0151533 , -0.00260982,  0.02325839,  0.00457436]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n",
       "array([[[0.20131408],\n",
       "        [0.20278044],\n",
       "        [0.19993325],\n",
       "        [0.1975839 ],\n",
       "        [0.19838838]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "albert_ipynb",
   "language": "python",
   "name": "env_bert-like"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
