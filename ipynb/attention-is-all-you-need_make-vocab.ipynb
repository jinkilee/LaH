{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation for kor → eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../attention-is-all-you-need/')\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import random\n",
    "import numpy as np\n",
    "from setting import *\n",
    "from dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: 187th sentence of /heavy_data/jkfirst/workspace/git/LaH/dataset/Corpus10/cekcorpus31.txt: 'cp949' codec can't decode byte 0xc9 in position 7412: illegal multibyte sequence\n",
      ">> 61094 sentence pairs loaded\n",
      ">> 62094 sentence pairs loaded\n"
     ]
    }
   ],
   "source": [
    "sent_pairs = load_dataset(path='/heavy_data/jkfirst/workspace/git/LaH/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['우리는 춘부장께 도움을 청해야 한다', 'we must ask help from your father']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spm-src-5000', 'spm-trg-3000')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = 5000\n",
    "trg_vocab_size = 3000\n",
    "src_prefix = 'spm-src-{}'.format(src_vocab_size)\n",
    "trg_prefix = 'spm-trg-{}'.format(trg_vocab_size)\n",
    "src_prefix, trg_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--input=./spm_src.txt --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --model_prefix=spm-src-5000 --vocab_size=5000 --character_coverage=1.0 --model_type=unigram'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_cmd = templates.format(\n",
    "    src_input_file,\n",
    "    pad_id,\n",
    "    bos_id,\n",
    "    eos_id,\n",
    "    unk_id,\n",
    "    src_prefix,\n",
    "    src_vocab_size,\n",
    "    character_coverage,\n",
    "    model_type)\n",
    "src_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--input=./spm_trg.txt --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --model_prefix=spm-trg-3000 --vocab_size=3000 --character_coverage=1.0 --model_type=unigram'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_cmd = templates.format(\n",
    "    trg_input_file,\n",
    "    pad_id,\n",
    "    bos_id,\n",
    "    eos_id,\n",
    "    unk_id,\n",
    "    trg_prefix,\n",
    "    trg_vocab_size,\n",
    "    character_coverage,\n",
    "    model_type)\n",
    "trg_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(src_cmd)\n",
    "spm.SentencePieceTrainer.Train(trg_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_spm = spm.SentencePieceProcessor()\n",
    "trg_spm = spm.SentencePieceProcessor()\n",
    "src_spm.Load('{}.model'.format(src_prefix)) \n",
    "trg_spm.Load('{}.model'.format(trg_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>\\t0',\n",
       " '<s>\\t0',\n",
       " '</s>\\t0',\n",
       " '<unk>\\t0',\n",
       " '▁\\t-2.86683',\n",
       " '을\\t-3.56501',\n",
       " '의\\t-3.78798',\n",
       " '를\\t-3.89044',\n",
       " '는\\t-4.03717',\n",
       " '가\\t-4.09034']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('{}.vocab'.format(src_prefix), encoding='utf-8') as f:\n",
    "    src_vocab = [doc.strip() for doc in f]\n",
    "src_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>\\t0',\n",
       " '<s>\\t0',\n",
       " '</s>\\t0',\n",
       " '<unk>\\t0',\n",
       " '▁the\\t-2.87699',\n",
       " 's\\t-3.42794',\n",
       " '▁to\\t-3.7658',\n",
       " '▁a\\t-3.80983',\n",
       " '▁he\\t-4.07266',\n",
       " 'ed\\t-4.09187']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('{}.vocab'.format(trg_prefix), encoding='utf-8') as f:\n",
    "    trg_vocab = [doc.strip() for doc in f]\n",
    "trg_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁질',\n",
       " '에',\n",
       " '▁있어서',\n",
       " '▁우리는',\n",
       " '▁다른',\n",
       " '▁상',\n",
       " '표',\n",
       " '의',\n",
       " '▁상품',\n",
       " '은',\n",
       " '▁우리',\n",
       " '의',\n",
       " '▁것과',\n",
       " '▁비교',\n",
       " '할',\n",
       " '▁수',\n",
       " '▁없다',\n",
       " '고',\n",
       " '▁생각합니다']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(sent_pairs))\n",
    "idx\n",
    "src_spm.EncodeAsPieces(sent_pairs[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁the', '▁woman', '▁can', '▁out', 's', 'we', 'ar', '▁her', '▁husband']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(sent_pairs))\n",
    "trg_spm.EncodeAsPieces(sent_pairs[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁',\n",
       "  '그는',\n",
       "  '▁',\n",
       "  '상류계층에서',\n",
       "  '▁',\n",
       "  '활동하',\n",
       "  '여',\n",
       "  '▁',\n",
       "  '교제하는',\n",
       "  '▁',\n",
       "  '사람은',\n",
       "  '▁',\n",
       "  '모두',\n",
       "  '▁',\n",
       "  '귀족',\n",
       "  '이',\n",
       "  '다'],\n",
       " ['▁he',\n",
       "  '▁move',\n",
       "  's',\n",
       "  '▁in',\n",
       "  '▁very',\n",
       "  '▁ra',\n",
       "  're',\n",
       "  'f',\n",
       "  'ied',\n",
       "  '▁circle',\n",
       "  's',\n",
       "  '▁his',\n",
       "  '▁friends',\n",
       "  '▁are',\n",
       "  '▁all',\n",
       "  '▁lord',\n",
       "  's'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(sent_pairs))\n",
    "src_sent = trg_spm.EncodeAsPieces(sent_pairs[idx][0])\n",
    "trg_sent = trg_spm.EncodeAsPieces(sent_pairs[idx][1])\n",
    "src_sent, trg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_list = [10000, 20000, 30000, 40000, 50000, 60000, 63000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences -> 22747 words\n",
      "20000 sentences -> 36729 words\n",
      "30000 sentences -> 48369 words\n",
      "40000 sentences -> 58464 words\n",
      "50000 sentences -> 67386 words\n",
      "60000 sentences -> 75547 words\n",
      "63000 sentences -> 77264 words\n"
     ]
    }
   ],
   "source": [
    "for n_max in n_max_list:\n",
    "    # read files\n",
    "    with open(src_input_file) as f:\n",
    "        sents = []\n",
    "        for oneline in f:\n",
    "            sents.append(oneline.rstrip())\n",
    "    \n",
    "    # shuffle and collect sents\n",
    "    random.shuffle(sents)\n",
    "    sents = sents[:n_max]\n",
    "    \n",
    "    # make vocab\n",
    "    src_word = {}\n",
    "    for s in sents:\n",
    "        s = s.split()\n",
    "        for si in s:\n",
    "            try:\n",
    "                src_word[si] += 1\n",
    "            except KeyError:\n",
    "                src_word[si] = 1\n",
    "                \n",
    "    print('{} sentences -> {} words'.format(n_max, len(src_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences -> 10265 words\n",
      "20000 sentences -> 15302 words\n",
      "30000 sentences -> 19130 words\n",
      "40000 sentences -> 22457 words\n",
      "50000 sentences -> 25282 words\n",
      "60000 sentences -> 27831 words\n",
      "63000 sentences -> 28369 words\n"
     ]
    }
   ],
   "source": [
    "for n_max in n_max_list:\n",
    "    # read files\n",
    "    with open(trg_input_file) as f:\n",
    "        sents = []\n",
    "        for oneline in f:\n",
    "            sents.append(oneline.rstrip())\n",
    "    \n",
    "    # shuffle and collect sents\n",
    "    random.shuffle(sents)\n",
    "    sents = sents[:n_max]\n",
    "    \n",
    "    # make vocab\n",
    "    trg_word = {}\n",
    "    for s in sents:\n",
    "        s = s.split()\n",
    "        for si in s:\n",
    "            try:\n",
    "                trg_word[si] += 1\n",
    "            except KeyError:\n",
    "                trg_word[si] = 1\n",
    "                \n",
    "    print('{} sentences -> {} words'.format(n_max, len(trg_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 503)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word['나'], trg_word['am']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
