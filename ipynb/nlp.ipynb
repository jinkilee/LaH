{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.insert(0, '/anything/git/asa/CERT/source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reference\n",
    "- https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da\n",
    "- https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, confusion_matrix\n",
    "from utils import fix_torch_randomness\n",
    "from feature import InputExample, InputFeatures\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### movie 데이터셋을 pd.DataFrame으로 로드 (https://ai.stanford.edu/~amaas/data/sentiment/에서 download 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['127_7.txt', '126_10.txt', '125_7.txt', '124_10.txt', '123_10.txt'],\n",
       " ['127_4.txt', '126_1.txt', '125_1.txt', '124_2.txt', '123_1.txt'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_filename = os.listdir('/data/asa/imdb/aclImdb/train/pos')\n",
    "negative_filename = os.listdir('/data/asa/imdb/aclImdb/train/neg')\n",
    "positive_filename[:5], negative_filename[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_filename = [os.path.join('/data/asa/imdb/aclImdb/train/pos', f) for f in positive_filename]\n",
    "negative_filename = [os.path.join('/data/asa/imdb/aclImdb/train/neg', f) for f in negative_filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/asa/imdb/aclImdb/train/pos/127_7.txt',\n",
       " '/data/asa/imdb/aclImdb/train/pos/126_10.txt',\n",
       " '/data/asa/imdb/aclImdb/train/pos/125_7.txt',\n",
       " '/data/asa/imdb/aclImdb/train/pos/124_10.txt',\n",
       " '/data/asa/imdb/aclImdb/train/pos/123_10.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_filename[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_df(files, label):\n",
    "    df = {\n",
    "        'text':[open(f, 'r').readline() for f in files],\n",
    "        'label': [label]*len(files)\n",
    "    }\n",
    "    df = pd.DataFrame(df)[['text','label']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = files_to_df(positive_filename, 1)\n",
    "neg_df = files_to_df(negative_filename, 0)\n",
    "df = pd.concat([pos_df, neg_df])\n",
    "df = df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12386</th>\n",
       "      <td>A masterpiece of comedy, a masterpiece of horr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>I had hoped this movie was going to be mildly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>Apparently Ruggero Deodato figured out, early ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>The Slackers as titled in this movie are three...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9674</th>\n",
       "      <td>First love is a desperately difficult subject ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>I purchased this film on the cheap in a sale, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>This is one of the best made movies from 2002....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>New York, I Love You finally makes it to our s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>This could be looked at in many different ways...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>I happily admit that I'm a sucker for a beauti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>Just okay horror film about a nice suburban fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>Outside the household is a different world and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>I bought this cheap from the rental remnant at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>You know you're in for something different whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Warning Spoilers following. Superb recreation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "12386  A masterpiece of comedy, a masterpiece of horr...      1\n",
       "3950   I had hoped this movie was going to be mildly ...      0\n",
       "5356   Apparently Ruggero Deodato figured out, early ...      1\n",
       "8972   The Slackers as titled in this movie are three...      0\n",
       "9674   First love is a desperately difficult subject ...      1\n",
       "541    I purchased this film on the cheap in a sale, ...      0\n",
       "9542   This is one of the best made movies from 2002....      1\n",
       "2914   New York, I Love You finally makes it to our s...      1\n",
       "1870   This could be looked at in many different ways...      0\n",
       "4399   I happily admit that I'm a sucker for a beauti...      1\n",
       "5196   Just okay horror film about a nice suburban fa...      0\n",
       "4592   Outside the household is a different world and...      1\n",
       "2220   I bought this cheap from the rental remnant at...      1\n",
       "8008   You know you're in for something different whe...      1\n",
       "225    Warning Spoilers following. Superb recreation ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 함수 정의 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = re.compile(r'<br \\/>', flags=re.IGNORECASE)\n",
    "nt = re.compile(r'n\\'t', flags=re.IGNORECASE)\n",
    "wd = re.compile(r'\\'d', flags=re.IGNORECASE)\n",
    "nb = re.compile(r'(\\s(\\-|)\\d+)')\n",
    "special_chars = [';',':','\"','\\'','(',')','{','}','\\.',',','\\?','/','!','@','#','$','%','^','&','\\*','\\-','\\+','=','\\r\\n','\\n','\\\\\\\\','`','>','<','~']\n",
    "sp_pattern = '[{}]'.format('|'.join(special_chars))\n",
    "sp = re.compile(sp_pattern)\n",
    "def preprocess(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub(br, ' ', x)\n",
    "    x = re.sub(nb, ' [number] ', x)\n",
    "    x = re.sub(nt, ' not ', x)\n",
    "    x = re.sub(wd, ' would ', x)\n",
    "    x = re.sub(sp, ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zentropa has much in [number]  common with the third man  another noir like film set among the rubble of postwar europe  like ttm  there is much inventive camera work  there is an innocent american who gets emotionally involved with a woman he does not  really understand  and whose naivety is all the more striking in contrast with the natives   but i would  have to say that the third man has a more well crafted storyline  zentropa is a bit disjointed in this respect  perhaps this is intentional  it is presented as a dream nightmare  and making it too coherent would spoil the effect    this movie is unrelentingly grim   noir  in more than one sense  one never sees the sun shine  grim  but intriguing  and frightening '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'Zentropa has much in 333 common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn\\'t really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I\\'d have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.'\n",
    "preprocess(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:01<00:00, 13815.72it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df['text'] = df.text.progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12386</th>\n",
       "      <td>a masterpiece of comedy  a masterpiece of horr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>i had hoped this movie was going to be mildly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>apparently ruggero deodato figured out  early ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>the slackers as titled in this movie are three...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9674</th>\n",
       "      <td>first love is a desperately difficult subject ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>i purchased this film on the cheap in a sale  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>this is one of the best made movies from [numb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>new york  i love you finally makes it to our s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>this could be looked at in many different ways...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>i happily admit that i m a sucker for a beauti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "12386  a masterpiece of comedy  a masterpiece of horr...      1\n",
       "3950   i had hoped this movie was going to be mildly ...      0\n",
       "5356   apparently ruggero deodato figured out  early ...      1\n",
       "8972   the slackers as titled in this movie are three...      0\n",
       "9674   first love is a desperately difficult subject ...      1\n",
       "541    i purchased this film on the cheap in a sale  ...      0\n",
       "9542   this is one of the best made movies from [numb...      1\n",
       "2914   new york  i love you finally makes it to our s...      1\n",
       "1870   this could be looked at in many different ways...      0\n",
       "4399   i happily admit that i m a sucker for a beauti...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word(texts):\n",
    "    vocab = {}\n",
    "    for text in tqdm(texts):\n",
    "        text = text.split()\n",
    "        for word in text:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:00<00:00, 29571.48it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = make_word(df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>163113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>masterpiece</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>145864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comedy</td>\n",
       "      <td>3246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horror</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>romance</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>if</td>\n",
       "      <td>16807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>there</td>\n",
       "      <td>18858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is</td>\n",
       "      <td>110502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anything</td>\n",
       "      <td>2947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word     cnt\n",
       "0            a  163113\n",
       "1  masterpiece     613\n",
       "2           of  145864\n",
       "3       comedy    3246\n",
       "4       horror    3591\n",
       "5      romance     694\n",
       "6           if   16807\n",
       "7        there   18858\n",
       "8           is  110502\n",
       "9     anything    2947"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df = pd.DataFrame(data=[(k, v) for k, v in vocab.items()], columns=['word','cnt'])\n",
    "vocab_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75280, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Naive-Bayes로 간단한 classifier 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature(단어) 선택해서 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_picked_word = 'happy,sad,gloomy,funny,beautiful,ugly,handsome,great,good,bad,awesome,worst,best'.split(',')\n",
    "len(hand_picked_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  4.01it/s]\n"
     ]
    }
   ],
   "source": [
    "for w in tqdm(hand_picked_word):\n",
    "    df[w] = df.text.apply(lambda x: 1 if w in x.split() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "      <th>gloomy</th>\n",
       "      <th>funny</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>ugly</th>\n",
       "      <th>handsome</th>\n",
       "      <th>great</th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>awesome</th>\n",
       "      <th>worst</th>\n",
       "      <th>best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9424</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       happy  sad  gloomy  funny  beautiful  ugly  handsome  great  good  bad  \\\n",
       "9424       0    0       0      0          0     0         0      1     0    1   \n",
       "4972       0    0       0      0          0     0         0      0     0    0   \n",
       "10195      0    0       0      0          0     0         0      1     0    0   \n",
       "9991       0    0       0      0          0     0         0      1     0    0   \n",
       "11735      0    0       0      1          0     0         0      0     1    0   \n",
       "\n",
       "       awesome  worst  best  \n",
       "9424         0      0     0  \n",
       "4972         0      0     0  \n",
       "10195        0      0     0  \n",
       "9991         0      0     0  \n",
       "11735        0      0     1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[hand_picked_word].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "      <th>gloomy</th>\n",
       "      <th>funny</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>ugly</th>\n",
       "      <th>handsome</th>\n",
       "      <th>great</th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>awesome</th>\n",
       "      <th>worst</th>\n",
       "      <th>best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9424</th>\n",
       "      <td>my kids picked this out at the video store   i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>we  as a family  were so delighted with  the l...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>this great movie has failed to register a high...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>the performance by om puri  smita patil  and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11735</th>\n",
       "      <td>down to earth is the best movie    it is so fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  happy  sad  \\\n",
       "9424   my kids picked this out at the video store   i...      0      0    0   \n",
       "4972   we  as a family  were so delighted with  the l...      1      0    0   \n",
       "10195  this great movie has failed to register a high...      1      0    0   \n",
       "9991   the performance by om puri  smita patil  and s...      1      0    0   \n",
       "11735  down to earth is the best movie    it is so fu...      1      0    0   \n",
       "\n",
       "       gloomy  funny  beautiful  ugly  handsome  great  good  bad  awesome  \\\n",
       "9424        0      0          0     0         0      1     0    1        0   \n",
       "4972        0      0          0     0         0      0     0    0        0   \n",
       "10195       0      0          0     0         0      1     0    0        0   \n",
       "9991        0      0          0     0         0      1     0    0        0   \n",
       "11735       0      1          0     0         0      0     1    0        0   \n",
       "\n",
       "       worst  best  \n",
       "9424       0     0  \n",
       "4972       0     0  \n",
       "10195      0     0  \n",
       "9991       0     0  \n",
       "11735      0     1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 15)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = hand_picked_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 15), (5000, 15))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(df.shape[0] * 0.8)\n",
    "train_df = df[:n_train]\n",
    "test_df = df[n_train:]\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier와 AdaBoostClassifier 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = RandomForestClassifier(n_estimators=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=11,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.fit(train_df[features], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.6151\n",
      "pre: 0.6587\n",
      "auc: 0.6486\n"
     ]
    }
   ],
   "source": [
    "pred = m1.predict(test_df[features])\n",
    "rec = recall_score(test_df['label'], pred)\n",
    "pre = precision_score(test_df['label'], pred)\n",
    "auc = roc_auc_score(test_df['label'], pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = AdaBoostClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(train_df[features], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.6039\n",
      "pre: 0.6708\n",
      "auc: 0.6541\n"
     ]
    }
   ],
   "source": [
    "pred = m2.predict(test_df[features])\n",
    "rec = recall_score(test_df['label'], pred)\n",
    "pre = precision_score(test_df['label'], pred)\n",
    "auc = roc_auc_score(test_df['label'], pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두 개의 모델 ensemble 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1 = m1.predict_proba(test_df[features])\n",
    "prob_2 = m2.predict_proba(test_df[features])\n",
    "prob = (prob_1 + prob_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.6151\n",
      "pre: 0.6589\n",
      "auc: 0.6488\n"
     ]
    }
   ],
   "source": [
    "pred = (prob[:,1]>0.5).astype(int)\n",
    "rec = recall_score(test_df['label'], pred)\n",
    "pre = precision_score(test_df['label'], pred)\n",
    "auc = roc_auc_score(test_df['label'], pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) deep leanrning + machine learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.text.apply(lambda x: x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.3 s, sys: 67.9 ms, total: 32.4 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드벡터를 이용해서 문장을 벡터화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector(text):\n",
    "    text = text.split()\n",
    "    vector = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            vector.append(w2v[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    vector = np.array(vector)\n",
    "    return vector.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "100%|██████████| 25000/25000 [00:20<00:00, 1205.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#generate_vector('i am so happy with it')\n",
    "x = df.text.progress_apply(generate_vector).values.tolist()\n",
    "x = np.array(x).reshape((len(x), 100))\n",
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 100), (25000,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(x) * 0.8)\n",
    "train_x = x[:n_train]\n",
    "train_y = y[:n_train]\n",
    "test_x = x[n_train:]\n",
    "test_y = y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 새로운 피처를(워드벡터) 이용해서 RandomForestClassifier와 AdaBoostClassifier 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = RandomForestClassifier(n_estimators=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=11,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.7765\n",
      "pre: 0.7461\n",
      "auc: 0.7564\n"
     ]
    }
   ],
   "source": [
    "pred = m1.predict(test_x)\n",
    "rec = recall_score(test_y, pred)\n",
    "pre = precision_score(test_y, pred)\n",
    "auc = roc_auc_score(test_y, pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = AdaBoostClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.8022\n",
      "pre: 0.7933\n",
      "auc: 0.7968\n"
     ]
    }
   ],
   "source": [
    "pred = m2.predict(test_x)\n",
    "rec = recall_score(test_y, pred)\n",
    "pre = precision_score(test_y, pred)\n",
    "auc = roc_auc_score(test_y, pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1 = m1.predict_proba(test_x)\n",
    "prob_2 = m2.predict_proba(test_x)\n",
    "prob = (prob_1 + prob_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = (prob[:,1]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.7765\n",
      "pre: 0.7461\n",
      "auc: 0.7564\n"
     ]
    }
   ],
   "source": [
    "rec = recall_score(test_y, pred1)\n",
    "pre = precision_score(test_y, pred1)\n",
    "auc = roc_auc_score(test_y, pred1)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) only deep learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocabulary 선택하기 및 패딩/UNK 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab = vocab_df[vocab_df.cnt>5].word.values\n",
    "my_vocab_dict = {v:i+1 for i,v in enumerate(my_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26416"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_dict['UNK'] = len(my_vocab) + 1\n",
    "my_vocab_dict['PAD'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_length(batches):\n",
    "    padidx = my_vocab_dict['PAD']\n",
    "    reallen_list = [(batch.tolist()+[padidx]).index(padidx) for batch in batches]\n",
    "    reallen_list = np.array(reallen_list)\n",
    "    return reallen_list\n",
    "\n",
    "def load_embeddings(word2index, n_dimen):\n",
    "    word_embeddings = {}\n",
    "    for word in word2index:\n",
    "        word_embeddings[word] = np.random.uniform(-0.25, 0.25, n_dimen)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNNBIRNN(object):\n",
    "    def __init__(self, hdnsize, maxword, mxpsize, n_class, n_dimen, fltsize, n_filts, vocab_size, l2regld=0.0):\n",
    "\n",
    "        # input placeholders\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, maxword], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, n_class], name='input_y')\n",
    "        self.keep_pr = tf.placeholder(tf.float32, name='keep_pr')\n",
    "        self.n_input = tf.placeholder(tf.int32, [], name='n_input')\n",
    "        self.reallen = tf.placeholder(tf.int32, [None], name='reallen')\n",
    "        self.padding = tf.placeholder(tf.float32, [None, 1, n_dimen, 1], name='padding')\n",
    "\n",
    "        self.a = tf.Variable(tf.random_uniform([vocab_size, n_dimen], -0.1, 0.1), name='embedded_matrix')\n",
    "        self.b = tf.nn.dropout(self.a, self.keep_pr, name='embedded_dropout_matrix')\n",
    "        self.c = tf.nn.embedding_lookup(self.b, self.input_x, name='embedded_w')\n",
    "        self.embedded_w = tf.expand_dims(self.c, -1, name='embedded_expanded_w')\n",
    "\n",
    "        reduced = np.int32(np.ceil((maxword) * 1.0 / mxpsize))\n",
    "\n",
    "        # construct TextCNN\n",
    "        pooled_outputs = []\n",
    "        for flt in fltsize:\n",
    "            # zero paddings\n",
    "            num_prio = (flt-1) // 2\n",
    "            num_post = (flt-1) - num_prio\n",
    "            pad_prio = tf.concat([self.padding] * num_prio, 1)\n",
    "            pad_post = tf.concat([self.padding] * num_post, 1)\n",
    "            #pad_prio = tf.concat([padding] * num_prio, 1)\n",
    "            #pad_post = tf.concat([padding] * num_post, 1)\n",
    "            emb_pad = tf.concat([pad_prio, self.embedded_w, pad_post], 1)\n",
    "\n",
    "            # convolution\n",
    "            filter_shape = [flt, n_dimen, 1, n_filts]\n",
    "            conv_w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='conv_w')\n",
    "            conv_b = tf.Variable(tf.constant(0.1, shape=[n_filts]), name='conv_b')\n",
    "            conv = tf.nn.conv2d(emb_pad, conv_w, strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
    "\n",
    "            # relu activation\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, conv_b), name='relu')\n",
    "\n",
    "            # maxpooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                    h, ksize=[1, mxpsize, 1, 1],\\\n",
    "                    strides=[1, mxpsize, 1, 1],\\\n",
    "                    padding='SAME', name='pool'\n",
    "            )\n",
    "            pooled = tf.reshape(pooled, [-1, reduced, n_filts])\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        pooled_outputs = tf.concat(pooled_outputs,2)\n",
    "        self.pooled_outputs = tf.nn.dropout(pooled_outputs, self.keep_pr, name='pooled_outputs')\n",
    "\n",
    "        # construct Bidirectional LSTM\n",
    "        lstm_fwcell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hdnsize)\n",
    "        lstm_bwcell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hdnsize)\n",
    "        #lstm_fwcell = tf.contrib.rnn.BasicLSTMCell(num_units=hdnsize)\n",
    "        #lstm_bwcell = tf.contrib.rnn.BasicLSTMCell(num_units=hdnsize)\n",
    "        lstm_fwcell = tf.nn.rnn_cell.DropoutWrapper(lstm_fwcell, output_keep_prob=self.keep_pr)\n",
    "        lstm_bwcell = tf.nn.rnn_cell.DropoutWrapper(lstm_bwcell, output_keep_prob=self.keep_pr)\n",
    "        _fwinitial_state = lstm_fwcell.zero_state(self.n_input, tf.float32)\n",
    "        _bwinitial_state = lstm_bwcell.zero_state(self.n_input, tf.float32)\n",
    "\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in\\\n",
    "                    tf.split(self.pooled_outputs,num_or_size_splits=int(reduced),axis=1)]\n",
    "\n",
    "        self.outputs, _, _ = tf.nn.static_bidirectional_rnn(\\\n",
    "            lstm_fwcell, lstm_bwcell,\\\n",
    "            inputs,\\\n",
    "            initial_state_fw=_fwinitial_state,\n",
    "            initial_state_bw=_bwinitial_state,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # collect the appropriate last words into variable output (dimension = batch x n_dimen)\n",
    "        output = self.outputs[0]\n",
    "        with tf.variable_scope('Output'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            one = tf.ones([1, hdnsize*2], tf.float32, name='one')\n",
    "            for i in range(1,len(self.outputs)):\n",
    "                ind = self.reallen < (i+1)\n",
    "                ind = tf.to_float(ind, name='ind_tofloat')\n",
    "                ind = tf.expand_dims(ind, -1, name='ind_expanded')\n",
    "                mat = tf.matmul(ind, one, name='output_matmul')\n",
    "                output = tf.add(tf.multiply(output, mat),tf.multiply(self.outputs[i], 1.0 - mat), name='output')\n",
    "\n",
    "        # define l2loss\n",
    "        l2loss = tf.constant(0.0)\n",
    "\n",
    "        # define output weights and bias for prediction\n",
    "        output_w = tf.Variable(tf.truncated_normal([hdnsize*2, n_class], stddev=0.1), name='output_w')\n",
    "        output_b = tf.Variable(tf.constant(0.1, shape=[n_class]), name='output_b')\n",
    "        l2loss = tf.nn.l2_loss(output_w) + l2loss\n",
    "        l2loss = tf.nn.l2_loss(output_b) + l2loss\n",
    "\n",
    "        # predict data\n",
    "        self.scores = tf.nn.xw_plus_b(output, output_w, output_b, name='scores')\n",
    "        #self.scores = tf.matmul(output, output_w, name='scores')\n",
    "        self.predictions = tf.argmax(self.scores, 1, name='predictions')\n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.scores)\n",
    "        #losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.scores)\n",
    "        correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "        self.loss = tf.reduce_mean(losses) + l2regld*l2loss\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1017 17:18:20.060177 140220720973568 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W1017 17:18:20.060735 140220720973568 deprecation.py:506] From <ipython-input-59-fb5cb373895c>:13: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1017 17:18:20.156929 140220720973568 deprecation.py:323] From <ipython-input-59-fb5cb373895c>:53: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1017 17:18:20.189847 140220720973568 deprecation.py:323] From <ipython-input-59-fb5cb373895c>:70: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
      "W1017 17:18:20.190348 140220720973568 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:1610: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "W1017 17:18:20.209116 140220720973568 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "W1017 17:18:20.215746 140220720973568 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1017 17:18:21.431387 140220720973568 deprecation.py:323] From <ipython-input-59-fb5cb373895c>:80: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1017 17:18:21.576535 140220720973568 deprecation.py:323] From <ipython-input-59-fb5cb373895c>:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = (vocab_df.cnt>5).sum()\n",
    "model = TextCNNBIRNN(\n",
    "    hdnsize=100,\n",
    "    maxword=200,\n",
    "    mxpsize=5,\n",
    "    n_class=2,\n",
    "    n_dimen=100,\n",
    "    fltsize=[3,4,5],\n",
    "    n_filts=128,\n",
    "    vocab_size=vocab_size,\n",
    "    l2regld=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1017 17:18:26.559751 140220720973568 deprecation.py:506] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# define Training procedure\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, decay=0.9)\n",
    "grads_and_vars = optimizer.compute_gradients(model.loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텐서플로우 초기화 및 학습/검증에 필요한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    text = text.split()\n",
    "    emb = []\n",
    "    for word in text[:200]:\n",
    "        try:\n",
    "            emb.append(my_vocab_dict[word])\n",
    "        except KeyError:\n",
    "            emb.append(my_vocab_dict['UNK'])\n",
    "            \n",
    "    if len(text) < 200:\n",
    "        emb += [my_vocab_dict['PAD']] * (200-len(text))\n",
    "    emb = np.array(emb)    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dimen = 100\n",
    "def train_model(images, labels):\n",
    "    # single training step\n",
    "    feed_dict = {\n",
    "        model.input_x: images,\n",
    "        model.input_y: labels,\n",
    "        model.padding: np.zeros([len(images), 1, n_dimen, 1]),\n",
    "        model.reallen: real_length(images),\n",
    "        model.n_input: len(images),\n",
    "        model.keep_pr: 0.6\n",
    "    }\n",
    "    _, step, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, model.loss, model.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    return loss, accuracy\n",
    "    #print('{}: step {}, loss {:.6f}, acc {:.4f}'.format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_model(images, labels, writer=None, rwords=None):\n",
    "    # evaluates model on a dev set\n",
    "    feed_dict = {\n",
    "        model.input_x: images,\n",
    "        model.input_y: labels,\n",
    "        model.padding: np.zeros([len(images), 1, n_dimen, 1]),\n",
    "        model.reallen: real_length(images),\n",
    "        model.n_input: len(images),\n",
    "        model.keep_pr: 1.0\n",
    "    }\n",
    "    step, predictions = sess.run(\n",
    "        [global_step, model.predictions],\n",
    "        feed_dict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제 학습 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    ################################\n",
    "    batch_size = 256\n",
    "    start = 0\n",
    "    tbar = trange(1, 2000)\n",
    "    for step in tbar:\n",
    "        if start >= train_df.shape[0]:\n",
    "            start = start % train_df.shape[0]\n",
    "        batch_x, batch_y = train_df[start:start+batch_size].text.apply(embed_text).values.tolist(), train_df[start:start+batch_size].label\n",
    "        batch_x = np.array(batch_x).reshape((len(batch_x), 200))\n",
    "        batch_y = np.eye(2)[batch_y]\n",
    "        loss, acc = train_model(batch_x, batch_y)\n",
    "        tbar.set_description('Tranining at {}: {:.4f} {:.4f}'.format(step, loss, acc))\n",
    "        start += batch_size\n",
    "        #break\n",
    "        \n",
    "    start = 0\n",
    "    batch_size = 1000\n",
    "    pred_list = []\n",
    "    while(1):\n",
    "        if start >= test_df.shape[0]:\n",
    "            break\n",
    "        batch_x, batch_y = test_df[start:start+batch_size].text.apply(embed_text).values.tolist(), test_df[start:start+batch_size].label\n",
    "        batch_x = np.array(batch_x).reshape((len(batch_x), 200))\n",
    "        batch_y = np.eye(2)[batch_y]\n",
    "        pred = dev_model(batch_x, batch_y)\n",
    "        pred_list += pred.tolist()\n",
    "        start += batch_size\n",
    "    pred_list = np.array(pred_list)\n",
    "    print(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred_list\n",
    "rec = recall_score(test_y, pred)\n",
    "pre = precision_score(test_y, pred)\n",
    "auc = roc_auc_score(test_y, pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) BERT 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3) only deep learning approach`에서 GPU를 사용하기 때문에 GPU를 Free한 후 실행해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(df.shape[0] * 0.8)\n",
    "train_df = df[:n_train]\n",
    "test_df = df[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config and define pretrained model\n",
    "config = BertConfig(\n",
    "    len(tok.vocab),\n",
    "    type_vocab_size=2,\n",
    "    half=False,\n",
    "    gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "fix_torch_randomness()\n",
    "model = BertForSequenceClassification(config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model.cuda()\n",
    "state_dict = torch.load('/data/asa/imdb_bert.bin')['state_dict']\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_df.label.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_example(txt, lbl, i):\n",
    "    return InputExample(guid=i, tokens_a=txt, tokens_b=None, label=lbl)\n",
    "\n",
    "examples = []\n",
    "for i, (txt, lbl) in enumerate(test_df[['text','label']].values):\n",
    "    examples.append(get_input_example(txt, lbl, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(tokens, vocab):\n",
    "    ids = []\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            ids.append(vocab[t])\n",
    "        except KeyError:\n",
    "            ids.append(vocab['[UNK]'])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_imdb_example_to_features(example, max_seq_length, tokenizer, do_display=False, do_mask=False):\n",
    "    pad_idx = tokenizer.vocab['[PAD]']\n",
    "    cls_idx = tokenizer.vocab['[CLS]']\n",
    "    sep_idx = tokenizer.vocab['[SEP]']\n",
    "\n",
    "    tokens = example.tokens_a\n",
    "    input_ids = [cls_idx] + text_to_ids(tokens.split(), tok.vocab)[:max_seq_length-2] + [sep_idx]\n",
    "    segment_ids = [1]*len(input_ids)\n",
    "    input_mask = [1]*len(input_ids)\n",
    "\n",
    "    if len(input_ids) < max_seq_length:\n",
    "        n_pad = max_seq_length - len(input_ids)\n",
    "        input_ids = input_ids + [pad_idx]*n_pad\n",
    "        segment_ids = segment_ids + [0]*n_pad\n",
    "        input_mask = input_mask + [0]*n_pad\n",
    "    else:\n",
    "        input_ids = input_ids[:max_seq_length]\n",
    "        segment_ids = segment_ids[:max_seq_length]\n",
    "        input_mask = input_mask[:max_seq_length]\n",
    "\n",
    "    assert len(input_ids) == max_seq_length, 'input_ids has invalid length of {}'.format(len(input_ids))\n",
    "    assert len(segment_ids) == max_seq_length, 'segment_ids has invalid length of {}'.format(len(segment_ids))\n",
    "    assert len(input_mask) == max_seq_length, 'input_mask has invalid length of {}'.format(len(input_mask))\n",
    "\n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                            input_mask=input_mask,\n",
    "                            segment_ids=segment_ids,\n",
    "                            lm_label_ids=None,\n",
    "                            is_next=None,\n",
    "                            label=example.label)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(map(lambda x: convert_imdb_example_to_features(x, 100, tok), examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def validate_classifier_model(model, valid_dataloader, threshold=0.5):\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # transform to evaluating mode\n",
    "    model.eval()\n",
    "\n",
    "    label_list = []\n",
    "    pred_list = []\n",
    "    tk0 = tqdm(valid_dataloader, desc='Evaluating')\n",
    "    for step, batch in enumerate(tk0):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        input_ids, segment_ids, input_mask, labels = batch\n",
    "\n",
    "        pred = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None)\n",
    "\n",
    "        if isinstance(pred, tuple):\n",
    "            _, pred = pred\n",
    "        pred = torch.sigmoid(pred) > threshold\n",
    "\n",
    "        labels = labels.cpu().numpy()\n",
    "        label_list.extend(labels.astype(int))\n",
    "        pred = pred.cpu().numpy()\n",
    "        pred_list.extend(pred)\n",
    "\n",
    "    return label_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 313/313 [00:14<00:00, 20.87it/s]\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = validate_classifier_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions).reshape(len(predictions), 2)\n",
    "test_df['prediction'] = predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3) (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape, predictions.shape)\n",
    "if isinstance(test_df.index, pd.MultiIndex):\n",
    "    print('re-indexed')\n",
    "    test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>damn  i thought i would  seen some bad western...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10477</th>\n",
       "      <td>this could have been interesting  a japan set...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>n b   spoilers within  assigning an artistic d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8815</th>\n",
       "      <td>all of you who despaired looking at the emptin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>the last hunt is one of the few westerns ever ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  prediction\n",
       "3725   damn  i thought i would  seen some bad western...      0           0\n",
       "10477  this could have been interesting  a japan set...      0           0\n",
       "3663   n b   spoilers within  assigning an artistic d...      0           0\n",
       "8815   all of you who despaired looking at the emptin...      1           1\n",
       "1242   the last hunt is one of the few westerns ever ...      1           1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec: 0.9762\n",
      "pre: 0.9754\n",
      "auc: 0.9760\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(predictions).reshape(test_df.shape[0], 2)[:,1]\n",
    "test_y = test_df['label']\n",
    "rec = recall_score(test_y, pred)\n",
    "pre = precision_score(test_y, pred)\n",
    "auc = roc_auc_score(test_y, pred)\n",
    "\n",
    "print('rec: {:.4f}'.format(rec))\n",
    "print('pre: {:.4f}'.format(pre))\n",
    "print('auc: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
